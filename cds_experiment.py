# -*- coding: utf-8 -*-
"""cds_experiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/ares2012/temp/blob/master/cds_experiment.ipynb

# ENV

<a target="_blank" href="https://colab.research.google.com/github/ares2012/temp/blob/master/Open_in_Colab.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

https://openincolab.com
"""

from google.colab import drive
drive.mount('/content/drive')

"""##### API_KEY"""

import os
import json

import numpy as np
import pandas as pd

cp -r /content/drive/MyDrive/Colab\ Notebooks/my_list.txt /content

file_path = '/content/my_list.txt'

with open(file_path, 'r') as f:
  keys = json.load(f)

oKEY = keys['OPENAI_API_KEY']
pKEY = keys['PINECONE_API_KEY']
hKEY = keys['huggingface.co']

!pip install openai --upgrade --quiet

"""##### Successfully installed transformers-4.57.3


* pip install captum bitsandbytes llmlingua
* pip install --upgrade transformers


"""

# ì„¸ì…˜ ë‹¤ì‹œ ì‹œì‘
! pip install captum bitsandbytes llmlingua

# pip install pycuda

# pip install --upgrade transformers

# pip install transformers==4.57.3

"""# CDS

"""

import os
import torch

import numpy as np
import pandas as pd
from scipy import stats

import matplotlib.pyplot as plt
import seaborn as sns

import random
from tqdm import tqdm

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import AutoConfig, BitsAndBytesConfig
from captum.attr import LayerIntegratedGradients, IntegratedGradients

os.environ["PYTORCH_CUDA_ALLOC_CON"] = "expandable_segments:True"
#os.environ["PYTORCH_NO_CUDA_MEMORY_CACHING"] = "1"
#os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# ! cp /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/072cb7562cb8c4adf682a8e186aaafa49469eb5d/configuration_phi3.py /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/f39ac1d28e925b323eae81227eaba4464caced4e

# ! cp /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/f39ac1d28e925b323eae81227eaba4464caced4e

# ==========================================
# 0. í™˜ê²½ ì„¤ì • ë° ì—£ì§€ ëª¨ë¸ ë¡œë“œ ë¡œì»¬ (1ë¶„)
# ==========================================
torch.cuda.empty_cache()
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

#model_name = "microsoft/Phi-3-mini-128k-instruct" #(1ë¶„, 57ì´ˆ/3.6G)
##model_name = "microsoft/Phi-3-mini-4k-instruct" #(4ë¶„, 52ì´ˆ)
model_name = "Qwen/Qwen3-0.6B" #Instruct (25ì´ˆ/1.3G, 6ì´ˆ/3.5G)
##model_name = "Qwen/Qwen3-0.6B-Base" #(25ì´ˆ/1.3G, 6ì´ˆ/3.5G)
#model_name = "Qwen/Qwen3-4B-Instruct-2507" #(25ì´ˆ/1.3G, 6ì´ˆ/3.5G)
local_path = '/content/drive/MyDrive/ColabNotebooks/hub/'

# Construct the path to the model's base directory in the hub
model_base_dir = os.path.join(local_path, "models--" + model_name.replace("/", "--"))

# Construct the path to the snapshots directory
snapshots_dir = os.path.join(model_base_dir, "snapshots")

# Dynamically get the snapshot hash (assuming there's only one subdirectory in snapshots)
if os.path.exists(snapshots_dir):
    snapshot_hashes = os.listdir(snapshots_dir)
    if snapshot_hashes:
        snapshot_hash = snapshot_hashes[0] # Take the first one found
        # Construct the full local path to the model directory including the dynamically found snapshot hash
        local_model_dir = os.path.join(snapshots_dir, snapshot_hash)
    else:
        raise FileNotFoundError(f"No snapshot hashes found in {snapshots_dir}")
else:
    raise FileNotFoundError(f"Snapshots directory not found: {snapshots_dir}")
print(f"Loading model from: {local_model_dir}")

# Explicitly load the configuration first
config = AutoConfig.from_pretrained(local_model_dir, trust_remote_code=True, local_files_only=True, output_attentions=True)
print(f"Type of config after loading: {type(config)}") # Debugging line

tokenizer = AutoTokenizer.from_pretrained(# model_name,
    local_model_dir, # Use the direct local path
    config=config, # Pass the loaded config
    #trust_remote_code=True
)
print(f"tokenizer.vocab_files_names: {tokenizer.vocab_files_names}")

bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
)
model = AutoModelForCausalLM.from_pretrained(# model_name,
        local_model_dir, # Use the direct local path
        config=config, # Pass the loaded config
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        attn_implementation="eager",
        #output_attentions=True,
        local_files_only = True # Ensure local files are used
)
# This globally disables the problematic caching mechanism.
model.config.use_cache = False

import gc
gc.collect(); torch.cuda.empty_cache()

torch.cuda.memory_reserved(0), torch.cuda.memory_allocated(0), torch.cuda.get_device_properties(0).total_memory

torch.cuda.mem_get_info(), torch.cuda.mem_get_info()[0]

"""##### í”„ë¡¬í”„íŠ¸


* apply chat model
* context+question


"""

# ==========================================
# 1. XAI: IG ë¶„ì„ ë° CDS ê³„ì‚°
# ==========================================
from captum.attr import IntegratedGradients
#from torch.cuda import AcceleratorError
#import pycuda.driver as cuda
def get_phi3_token_importance(context, question, model, tokenizer):
    # ëª…ì‹œì ì¸ êµ¬ì¡°ë¥¼ ê°€ì§„ í”„ë¡¬í”„íŠ¸ ìƒì„± (Separator ê°ì§€ ìš©ì´)
    # Phi-3 í…œí”Œë¦¿ + ëª…í™•í•œ êµ¬ë¶„ì
    full_text = f"<|user|>\nContext:\n{context}\n\nQuestion:\n{question}\n\nAnswer based on the context:<|end|>\n<|assistant|>"

    # 0. ì…ë ¥ ID ì¶”ì¶œ
    torch.cuda.empty_cache()
    inputs = tokenizer(full_text, return_tensors="pt", add_special_tokens=False).to(DEVICE)
    input_ids = inputs.input_ids
    torch.cuda.empty_cache()

    # 1. Raw Attention ì¶”ì¶œ (Forward Pass)
    with torch.no_grad():
      #output = model(**inputs)#; print(type(output))
      output = model(input_ids, output_attentions=True) #  Attentionë§Œ ì¶”ì¶œ
      # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì–´í…ì…˜ (Batch, Head, Seq, Seq) -> Head í‰ê·  -> (Seq, Seq)
      raw_attn = output.attentions[-1][0].mean(dim=0).cpu().numpy()
      #raw_attn = output.attentions[0][0].mean(dim=0).cpu().numpy()

    torch.cuda.empty_cache()
    # 2. ì„ë² ë”© ë ˆì´ì–´ ì°¾ê¸°
    if hasattr(model, "model") and hasattr(model.model, "embed_tokens"):
        embedding_layer = model.model.embed_tokens
    elif hasattr(model, "embed_tokens"): # Llama/Gemma ì¼ë¶€
        embedding_layer = model.embed_tokens
    else:
        embedding_layer = model.get_input_embeddings()

    torch.cuda.empty_cache()
    # 3. ì„ë² ë”© ë²¡í„°ë¥¼ ë¯¸ë¦¬ ê³„ì‚° > Captumì˜ Input
    # (4-bit ëª¨ë¸ ë“±ì—ì„œ Gradient ê³„ì‚°ì„ ìœ„í•´ requires_grad ì„¤ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ)
    input_embeddings = embedding_layer(input_ids)

    torch.cuda.empty_cache()
    # 4. Forward í•¨ìˆ˜ ì •ì˜ (ì„ë² ë”©ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ)
    def forward_func(inputs_embeds):
        # inputs_embedsë¥¼ ëª¨ë¸ì— ì§ì ‘ ì£¼ì…
        #outputs = model(inputs_embeds=inputs_embeds)
        outputs = model(inputs_embeds=inputs_embeds, output_attentions=False) # attention ë¹„í™œì„±í™”
        #return outputs.logits[0, -1, :].max().unsqueeze(0)
        return outputs.logits[0, -1].max().unsqueeze(0)

    try:
        # 5. [ë³€ê²½] LayerIntegratedGradients -> IntegratedGradients
        # ë ˆì´ì–´ê°€ ì•„ë‹ˆë¼ 'ì…ë ¥ í…ì„œ(input_embeddings)' ìì²´ë¥¼ ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ í•¨
        ig = IntegratedGradients(forward_func)

        torch.cuda.empty_cache()
        # 6. ì†ì„±(Attribute) ê³„ì‚°
        attributions, delta = ig.attribute(
            inputs=input_embeddings,
            # Baselineì€ 0 ë²¡í„° (All-zero embeddings)
            baselines=torch.zeros_like(input_embeddings),
            n_steps=1,
            internal_batch_size=1, # í•œ ë²ˆì— 1ê°œ ë‹¨ê³„ì”©ë§Œ ê³„ì‚°í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
            return_convergence_delta=True
        )
    except RuntimeError as e:
    #except (AcceleratorError, RuntimeError) as e:
    #except (cuda.Error, MemoryError, RuntimeError) as e:
      if "out of memory" in str(e):
        #del attributions, delta, inputs, input_ids, input_embeddings
        torch.cuda.empty_cache()
        print("Skipping sample due to OOM.")
        torch.cuda.empty_cache()
        return None, None, None, None
      else:
        raise e

    # 7. ì¤‘ìš”ë„ ì ìˆ˜ í•©ì‚° (L2 Norm)
    scores = torch.norm(attributions, dim=-1).squeeze().tolist()
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])

    # 8. CDS ê³„ì‚° (Context ì˜ì—­: 'Question:' ì „ê¹Œì§€)
    try:
        # í† í° ì¤‘ 'Question' ë¬¸ìì—´ì„ í¬í•¨í•˜ëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°
        sep_idx = next(i for i, t in enumerate(tokens) if "Question" in t)
        context_score = sum(scores[:sep_idx])
        total_score = sum(scores)
        cds = context_score / total_score if total_score > 0 else 0
    except StopIteration:
        cds = 0.5 # êµ¬ë¶„ì ëª» ì°¾ìŒ
        sep_idx = len(tokens) // 2

    del attributions, delta, inputs, input_ids, input_embeddings, output
    torch.cuda.empty_cache()
    #print(tokens, scores, cds, raw_attn)

    return tokens, scores, cds, raw_attn

import torch
from captum.attr import IntegratedGradients

MAX_CHUNK_TOKENS = 1024   # ë„ˆë¬´ í¬ë©´ IGì—ì„œ OOM â†’ chunk ë‹¨ìœ„ë¡œ ì²˜ë¦¬
DEVICE = "cuda"

def chunk_tokens(input_ids, chunk_size=MAX_CHUNK_TOKENS):
    chunks = []
    for i in range(0, len(input_ids), chunk_size):
        chunks.append(input_ids[i:i + chunk_size])
    return chunks

def get_phi3_token_importance(context, question, model, tokenizer):
    """
    ê¸¸ì´ 5000 ì´ìƒì˜ ë¬¸ì„œì—ì„œë„ OOMì´ ë°œìƒí•˜ì§€ ì•Šë„ë¡
    ì…ë ¥ì„ Chunk ë‹¨ìœ„ë¡œ ì˜ë¼ IG ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë²„ì „.
    """

    full_text = (
        "<|user|>\nContext:\n" +
        context +
        "\n\nQuestion:\n" +
        question +
        "\n\nAnswer based on the context:<|end|>\n<|assistant|>"
    )

    # -------------------------------------------------------
    # 1. í† í¬ë‚˜ì´ì§• (CPU ìœ ì§€)
    # -------------------------------------------------------
    torch.cuda.empty_cache()
    inputs = tokenizer(full_text, return_tensors="pt", add_special_tokens=False)
    input_ids = inputs.input_ids[0].tolist()  # list ë¡œ ë³€í™˜í•˜ì—¬ CPU ìœ ì§€

    # ë§¤ìš° ê¸´ ê²½ìš° chunkë¡œ ë¶„ë¦¬
    token_chunks = chunk_tokens(input_ids, MAX_CHUNK_TOKENS)

    # ì „ì²´ ê²°ê³¼ ì €ì¥ìš©
    all_tokens = []
    all_scores = []
    all_raw_attn = []

    # -------------------------------------------------------
    # 2. Attention ì¶”ì¶œ (Chunk ë‹¨ìœ„)
    # -------------------------------------------------------
    for c_idx, chunk in enumerate(token_chunks):
        input_tensor = torch.tensor(chunk, dtype=torch.long).unsqueeze(0).to(DEVICE)

        with torch.no_grad():
            out = model(input_tensor, output_attentions=True)
            raw_attn = out.attentions[-1][0].mean(dim=0).cpu().numpy()
            all_raw_attn.append(raw_attn)

        # GPU ë©”ëª¨ë¦¬ ì¦‰ì‹œ ë¹„ìš°ê¸°
        del out, input_tensor
        torch.cuda.empty_cache()

    # -------------------------------------------------------
    # 3. ì„ë² ë”© ë ˆì´ì–´ ê°€ì ¸ì˜¤ê¸°
    # -------------------------------------------------------
    if hasattr(model, "model") and hasattr(model.model, "embed_tokens"):
        embedding_layer = model.model.embed_tokens
    elif hasattr(model, "embed_tokens"):
        embedding_layer = model.embed_tokens
    else:
        embedding_layer = model.get_input_embeddings()

    # -------------------------------------------------------
    # 4. Chunk ë‹¨ìœ„ IG ê³„ì‚°
    # -------------------------------------------------------
    for c_idx, chunk in enumerate(token_chunks):

        # í† í° â†’ í…ì„œ
        ids_tensor = torch.tensor(chunk, dtype=torch.long).unsqueeze(0).to(DEVICE)

        # ì„ë² ë”© ê³„ì‚°
        input_embeds = embedding_layer(ids_tensor)

        # Forward í•¨ìˆ˜ (embedding ê¸°ë°˜)
        def forward_func(embeds):
            out = model(inputs_embeds=embeds, output_attentions=False)
            return out.logits[0, -1].max().unsqueeze(0)

        ig = IntegratedGradients(forward_func)

        try:
            attributions, delta = ig.attribute(
                inputs=input_embeds,
                baselines=torch.zeros_like(input_embeds),
                n_steps=1,
                internal_batch_size=1,
                return_convergence_delta=True
            )
        except RuntimeError as e:
            if "out of memory" in str(e):
                torch.cuda.empty_cache()
                print("Chunk", c_idx, "IG skipped due to OOM")
                continue
            else:
                raise e

        # L2 Norm ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚°
        scores = torch.norm(attributions, dim=-1).squeeze().tolist()
        tokens = tokenizer.convert_ids_to_tokens(chunk)

        all_tokens.extend(tokens)
        all_scores.extend(scores)

        del attributions, delta, ids_tensor, input_embeds
        torch.cuda.empty_cache()

    # -------------------------------------------------------
    # 5. CDS ê³„ì‚°
    # -------------------------------------------------------
    try:
        sep_idx = next(i for i, t in enumerate(all_tokens) if "Question" in t)
        context_score = sum(all_scores[:sep_idx])
        total_score = sum(all_scores)
        cds = context_score / total_score if total_score > 0 else 0
    except StopIteration:
        cds = 0.5
        sep_idx = len(all_tokens) // 2

    return all_tokens, all_scores, cds, all_raw_attn

#! cp -r /root/.cache/huggingface/hub/datasets--ms_marco/. /content/drive/MyDrive/ColabNotebooks/hub/datasets--ms_marco/

#! cp -r /root/.cache/huggingface/datasets/ms_marco/. /content/drive/MyDrive/ColabNotebooks/datasets/ms_marco/

#! cp -r /root/.cache/huggingface/hub/datasets--squad/. /content/drive/MyDrive/ColabNotebooks/hub/datasets--squad/

#! cp -r /root/.cache/huggingface/datasets/squad/. /content/drive/MyDrive/ColabNotebooks/datasets/squad/

# ==========================================
# 2. ë©”ì¸ ì‹¤í—˜ ë£¨í”„ (ë°ì´í„° ìˆ˜ì§‘) 3ì´ˆ
# ==========================================

#dataset = load_dataset('hotpot_qa', 'distractor', split="validation", cache_dir="/content/drive/MyDrive/ColabNotebooks/datasets")
#dataset1 = load_dataset('squad', 'plain_text', split="validation", cache_dir="/content/drive/MyDrive/ColabNotebooks/datasets")
#dataset2 = load_dataset('ms_marco', 'v2.1', split="validation", cache_dir="/content/drive/MyDrive/ColabNotebooks/datasets")
#dataset2 = dataset2.rename_column("query", "question").rename_column("passages", "context")
dataset3 = load_dataset('hotpot_qa', 'distractor', split="validation", cache_dir="/content/drive/MyDrive/ColabNotebooks/hub")

#dataset1 #dataset2 #
dataset3

#dataset3[0]['context']['sentences'][0]

#dataset2[0]['context']['passage_text']

#dataset1[0]['context']

results = []; NUM_SAMPLES = 200  # ë…¼ë¬¸ìš©ìœ¼ë¡œëŠ” 100~200ê°œ ê¶Œì¥(10ê°œ/1ë¶„)
dataset = dataset3.shuffle(seed=42).select(range(NUM_SAMPLES))

best_viz_candidate = None
max_cds_diff = -1
print(f"\nğŸš€ Collecting Data from {NUM_SAMPLES} samples...")

for i in tqdm(range(NUM_SAMPLES)):
    torch.cuda.empty_cache()#; print(torch.cuda.memory_allocated(0))
    data = dataset[i]
    question = data['question']
    # [ì¡°ê±´ 1] Faithful: ì˜¬ë°”ë¥¸ ë¬¸ë§¥
    cds_faithful = " ".join(["".join(sent) for sent in data['context']['sentences']])
    #cds_faithful = " ".join(["".join(sent) for sent in data['context']['passage_text']])
    #cds_faithful= data['context']
    #print("cds_faithful:",cds_faithful)
    # [ì¡°ê±´ 2] Distracted: ë¬´ê´€í•œ ë¬¸ë§¥ (ëœë¤ ìƒ˜í”Œë§)
    rand_idx = random.randint(0, len(dataset)-1)
    while rand_idx == i: rand_idx = random.randint(0, len(dataset)-1)
    cds_distracted = " ".join(["".join(sent) for sent in dataset[rand_idx]['context']['sentences']])
    #cds_distracted = " ".join(["".join(sent) for sent in dataset[rand_idx]['context']['passage_text']])
    #cds_distracted = dataset[rand_idx]['context']
    #print("cds_distracted:",cds_distracted)
    print(i, len(cds_faithful), len(cds_distracted))

    # ë¶„ì„ ì‹¤í–‰
    tok_f, score_f, cds_f, attn_f = get_phi3_token_importance(cds_faithful, question, model, tokenizer)
    tok_h, score_h, cds_h, attn_h = get_phi3_token_importance(cds_distracted, question, model, tokenizer)
    if cds_f is None or cds_h is None: continue # ì—ëŸ¬/OOM ìŠ¤í‚µ

    # ê²°ê³¼ ì €ì¥
    diff = cds_f - cds_h
    results.append({"id": i,"cds_faithful": cds_f,"len_faithful": len(cds_faithful),
                    "cds_distracted": cds_h, "len_distracted": len(cds_distracted), "diff": diff })
    # ì‹œê°í™”ìš© 'ìµœê³ ì˜ ìƒ˜í”Œ' ì €ì¥ (ì°¨ì´ê°€ ê°€ì¥ í° ê²ƒ)
    if diff > max_cds_diff:
      max_cds_diff = diff
      best_viz_candidate = {
          "tok_f": tok_f, "score_f": score_f, "attn_f": attn_f,
          "tok_h": tok_h, "score_h": score_h, "attn_h": attn_h,
          "q": question, "f": cds_faithful, "d": cds_distracted         }

# CSV ì €ì¥
df = pd.DataFrame(results)
file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_hotpot_qa_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_ms_marco_v2-1_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_squad_with_len.csv"

#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_hotpot_qa_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_ms_marco_v2-1_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_squad_with_len.csv"

df.to_csv(file_name, index=False)
df.to_csv("phase1_experiment_results.csv", index=False)
print("âœ… Phase1 Experiment results Saved.")

import json
import numpy as np

# Prepare a copy of the dictionary to modify for JSON serialization
# This prevents modifying the original best_viz_candidate if it's used elsewhere as numpy array
serializable_candidate = best_viz_candidate.copy()
serializable_candidate['attn_f'] = best_viz_candidate['attn_f'][0].tolist()
serializable_candidate['attn_h'] = best_viz_candidate['attn_h'][0].tolist()

file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_hotpot_qa_best_results.json"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_ms_marco_v2-1_best_results.json"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_squad_best_results.json"

#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_hotpot_qa_best_results.json"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_ms_marco_v2-1_best_results.json"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_squad_best_results.json"

with open(file_name, 'w') as f:
    json.dump(serializable_candidate, f)

type(best_viz_candidate), best_viz_candidate.keys()

type(best_viz_candidate['score_f']), len(best_viz_candidate['score_f']), best_viz_candidate['score_f'][0]

type(best_viz_candidate['attn_f']), len(best_viz_candidate['attn_f']), best_viz_candidate['attn_f'][0], len(best_viz_candidate['attn_f'][0])

# ==========================================
# 3. ì •ëŸ‰ ë¶„ì„: í†µê³„ ê²€ì • (T-test & kdeplot)
# ==========================================
import numpy as np
import pandas as pd
from scipy import stats

import matplotlib.pyplot as plt
import seaborn as sns

#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_hotpot_qa_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_ms_marco_v2-1_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_squad_with_len.csv"

file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_hotpot_qa_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_ms_marco_v2-1_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_squad_with_len.csv"

with open(file_name, 'r') as f:
    df = pd.read_csv(f); print(df.shape)
    #df = df[(df['len_faithful']< 1100) & (df['len_distracted'] < 1100)];  print(df.shape)
    #df = df[(df['len_faithful']< 3500) & (df['len_distracted'] < 3500)];  print(df.shape)
    #df = df[(df['len_faithful']>= 3500) | (df['len_distracted'] >= 3500)];  print(df.shape)
    #df = df[(df['len_faithful']< 4400) & (df['len_distracted'] < 4400)];  print(df.shape)
    #df = df[(df['len_faithful'] >= 4400) & (df['len_distracted'] >= 4400)];  print(df.shape)

    df['len_diff'] = df['len_faithful'] - df['len_distracted']
    #df = df[(df['len_diff']< 0)];  print(df.shape)

df.describe()

# ê¸¸ì´ ë¹„êµ
t_stat, p_val = stats.ttest_rel(df['len_faithful'], df['len_distracted'])
print("\n" + "="*50)
print("ğŸ“Š [Phase 1] Quantitative Results by length")
print("="*50)
print(f"Mean len (Faithful):     {df['len_faithful'].mean():.4f} (std: {df['len_faithful'].std():.4f})")
print(f"Mean len (Distracted): {df['len_distracted'].mean():.4f} (std: {df['len_distracted'].std():.4f})")
print(f"Gap (Faithful - Distracted): {df['len_diff'].mean():.4f}")
print("-" * 50)
print(f"Statistical Significance (Paired T-test):")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value:     {p_val:.4e}")
if p_val < 0.05:
    print("âœ… Result: Statistically Significant (p < 0.05)")
else:
    print("âŒ Result: Not Significant")

plt.figure(figsize=(10, 6))
sns.kdeplot(df['len_faithful'], label='CDS Faithful', fill=True, color='blue', alpha=0.5)
sns.kdeplot(df['len_distracted'], label='CDS Distracted', fill=True, color='orange', alpha=0.5)
plt.title('Comparison of Length between Faithful and Distracted Distributions')
plt.xlabel('Length')
plt.ylabel('Density')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()

def corr_len_cds_(corr_):
  cc7=0.7; cc5=0.5; cc3=0.3
  if abs(corr_) > cc7:
      print("âš ï¸ ê¸¸ì´ ì°¨ì´ê°€ CDS ì°¨ì´ì— ê°•ë ¥í•œ ì˜í–¥ì„ ì¤„ ê°€ëŠ¥ì„±ì´ ìˆìŒ.")
  if abs(corr_) > cc5:
      print("âš ï¸ ê¸¸ì´ ì°¨ì´ê°€ CDS ì°¨ì´ì— ì¤‘ê°„ ì˜í–¥ì„ ì¤„ ê°€ëŠ¥ì„±ì´ ìˆìŒ.")
  elif abs(corr_) > cc3:
      print("âš ï¸ ê¸¸ì´ ì°¨ì´ê°€ CDS ì°¨ì´ì— ì˜ë¯¸ ìˆëŠ” ì˜í–¥ì„ ì¤„ ê°€ëŠ¥ì„±ì´ ìˆìŒ.")
  else:
      print("â„¹ï¸ ê¸¸ì´ ì°¨ì´ì˜ ì˜í–¥ì€ í¬ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë³´ì„.")

corr_len_cds = df['len_diff'].corr(df['diff'])
print(f"Correlation(len_diff, cds_diff): {corr_len_cds:.4f}")
corr_len_cds_(corr_len_cds)
corr_faithful = df['len_faithful'].corr(df['cds_faithful'])
print(f"Correlation(len_f, cds_f): {corr_faithful:.4f}")
corr_len_cds_(corr_faithful)
corr_distracted = df['len_distracted'].corr(df['cds_distracted'])
print(f"Correlation(len_d, cds_d): {corr_distracted:.4f}")
corr_len_cds_(corr_distracted)

# CDS ë¹„êµ
t_stat, p_val = stats.ttest_rel(df['cds_faithful'], df['cds_distracted'])
print("\n" + "="*50)
print("ğŸ“Š [Phase 1] Quantitative Results")
print("="*50)
print(f"Mean CDS (Faithful):     {df['cds_faithful'].mean():.4f} (std: {df['cds_faithful'].std():.4f})")
print(f"Mean CDS (Distracted): {df['cds_distracted'].mean():.4f} (std: {df['cds_distracted'].std():.4f})")
print(f"Gap (Faithful - Distracted): {df['diff'].mean():.4f}")
print("-" * 50)
print(f"Statistical Significance (Paired T-test):")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value:     {p_val:.4e}")
if p_val < 0.05:
    print("âœ… Result: Statistically Significant (p < 0.05)")
else:
    print("âŒ Result: Not Significant")

plt.figure(figsize=(10, 6))
sns.kdeplot(df['cds_faithful'], label='CDS Faithful', fill=True, color='blue', alpha=0.5)
sns.kdeplot(df['cds_distracted'], label='CDS Distracted', fill=True, color='orange', alpha=0.5)
plt.title('Comparison of CDS Faithful and CDS Distracted Distributions')
plt.xlabel('Score')
plt.ylabel('Density')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()
file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/fig1_Comparison_Distributions.png"
#plt.savefig(file_name, dpi=300)
print("âœ… Figure 1 Saved.")

import json
file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_hotpot_qa_best_results.json"
with open(file_name, 'r') as f:
    best_result = json.load(f)

# Convert the loaded lists back to numpy arrays if they are intended to be used as such later
best_result['attn_f'] = np.array(best_result['attn_f'])
best_result['attn_h'] = np.array(best_result['attn_h'])

type(best_result['attn_f']), len(best_result['attn_f']), best_result['attn_f'][0]

"""##### ì‹œê°í™”

"""

# ==========================================
# 4. ì •ì„± ë¶„ì„ ì‹œê°í™” (Figure 1 & 2)
# ==========================================
if best_viz_candidate:
    sns.set_theme(style="white")
    plt.rcParams['font.family'] = 'serif'
    # --- Figure 1: Heatmap Comparison (Intensity) ---
    fig1, axes = plt.subplots(2, 1, figsize=(12, 5))

    def draw_heatmap(ax, tokens, scores, title):
      limit = 60
      clean = [t.replace(' ', '').replace('Ä ', '').replace('<', '').replace('>', '').replace('|', '').replace('user', '').replace('Context', '') for t in tokens if t != ''][7:limit]
      vals = np.array(scores)[7:limit].reshape(1, -1)
      vals = (vals - vals.min()) / (vals.max() - vals.min() + 1e-9) # Normalize

      sns.heatmap(vals, xticklabels=clean, yticklabels=False, cmap="Reds", ax=ax, cbar=False)
      ax.set_title(title, loc='left', fontsize=12, fontweight='bold')
      ax.set_xticklabels(clean, rotation=45, ha='right', fontsize=8)

    draw_heatmap(axes[0], best_viz_candidate['tok_f'], best_viz_candidate['score_f'], "(a) Faithful: High Attribution on Context")
    draw_heatmap(axes[1], best_viz_candidate['tok_h'], best_viz_candidate['score_h'], "(b) Distracted: Attribution Shifted to Question")

    plt.tight_layout()
    plt.savefig("fig1_heatmap.png", dpi=300)
    print("âœ… Figure 1 Saved.")

    # --- Figure 2: Attention Map Comparison (Structure) ---
    # ì–´í…ì…˜ ë§µì€ N x N ì´ë¯€ë¡œ ì˜ë¼ì„œ(Crop) ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ì¢‹ìŒ
    fig2, axes = plt.subplots(1, 2, figsize=(14, 6))

    def draw_attn_map(ax, tokens, attn_mat, title):
      limit = 40 # 40x40 í† í°ë§Œ ì‹œê°í™” (ê°€ë…ì„± ìœ„í•¨)
      clean = [t.replace(' ', '').replace('Ä ', '').replace('<', '').replace('>', '').replace('|', '').replace('user', '').replace('Context', '') for t in tokens if t != ''][7:limit]
      mat = attn_mat[7:limit, 7:limit]

      sns.heatmap(mat, xticklabels=clean, yticklabels=clean, cmap="Blues", ax=ax, cbar=False, square=True)
      ax.set_title(title, fontsize=12, fontweight='bold')
      ax.tick_params(axis='x', rotation=90, labelsize=7)
      ax.tick_params(axis='y', rotation=0, labelsize=7)

    draw_attn_map(axes[0], best_viz_candidate['tok_f'], best_viz_candidate['attn_f'], "(a) Faithful Attention Structure")
    draw_attn_map(axes[1], best_viz_candidate['tok_h'], best_viz_candidate['attn_h'], "(b) Distracted Attention Structure")

    plt.tight_layout()
    plt.savefig("fig2_attention_map.png", dpi=300)
    print("âœ… Figure 2 Saved.")

best_result['f']

best_result['q']

best_result['d']

# ==========================================
# 4. ì •ì„± ë¶„ì„: Figure 1 íˆíŠ¸ë§µ ìƒì„±
# ==========================================
if best_viz_candidate:
    print(f"\nğŸ¨ Generating Figure 1 from Sample ID (Best Gap)...")

    sns.set_theme(style="white")
    plt.rcParams['font.family'] = 'serif'
    fig, axes = plt.subplots(2, 1, figsize=(12, 5), sharex=False)
    def draw_row(ax, tokens, scores, title):
        # ì‹œê°í™” ê¸¸ì´ ì œí•œ
        limit = 70
        clean_toks = [t.replace(' ', '').replace('Ä ', '') for t in tokens][7:limit]
        norm_scores = np.array(scores)[7:limit]
        # ì •ê·œí™”
        if norm_scores.max() > 0:
            norm_scores /= norm_scores.max()
        sns.heatmap(
            norm_scores.reshape(1, -1),
            xticklabels=clean_toks, yticklabels=False,
            cmap="Reds", ax=ax, cbar=True,
            cbar_kws={"orientation": "vertical", "shrink": 0.8}
        )
        ax.set_title(title, fontsize=12, fontweight='bold', loc='left')
        ax.set_xticklabels(clean_toks, rotation=45, ha='right', fontsize=8)
    draw_row(axes[0], best_viz_candidate['tok_f'], best_viz_candidate['score_f'],
             f"(a) Faithful Generation (Correct Context) - High Attention on Context")
    draw_row(axes[1], best_viz_candidate['tok_h'], best_viz_candidate['score_h'],
             f"(b) Hallucination Risk (Irrelevant Context) - Attention Shifted to Question")

    plt.tight_layout()
    plt.savefig("phase1_figure1_heatmap.png", bbox_inches='tight', dpi=300)
    print("âœ… Saved 'phase1_figure1_heatmap.png'")

print("\nğŸ‰ Phase 1 Experiment Complete.")

import numpy as np
import matplotlib.pyplot as plt
from IPython.display import HTML, display

import re

def clean_token(tok):
    """
    ë¶ˆí•„ìš”í•œ BPE í† í° ì ‘ë‘ì–´(Ä , ÄŠ, â– ë“±)ì™€ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°/ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜.
    """

    # 1. BPE prefix ì œê±°
    tok = tok.replace("Ä ", " ")      # GPT-2 ê³„ì—´ ê³µë°± í† í°
    tok = tok.replace("ÄŠ", " ")      # Line-break ê³„ì—´
    tok = tok.replace("â–", " ")      # SentencePiece ê³µë°±

    # 2. HTML-safe ì²˜ë¦¬ (ì›í•˜ëŠ” ê²½ìš°)
    tok = tok.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

    # 3. ì–‘ìª½ ê³µë°± ì œê±°
    tok = tok.strip()

    # 4. íŠ¹ìˆ˜ë¬¸ì í´ë¦°ì—… (ë¬¸ì¥ë¶€í˜¸ëŠ” ìœ ì§€)
    tok = re.sub(r"[^A-Za-z0-9ê°€-í£ ,.?!%:/\-_=+()]", "", tok)

    # 5. ë‹¤ì‹œ í•œ ë²ˆ trim
    tok = tok.strip()

    return tok if tok != "" else " "

def split_tokens_to_sentences(tokens, scores):
    sentences = []
    curr_toks = []
    curr_scores = []

    end_tokens = [".", "?", "!"]

    for t, s in zip(tokens, scores):
        curr_toks.append(t)
        curr_scores.append(s)

        if t in end_tokens or t.endswith(tuple(end_tokens)):
            sentences.append((curr_toks, curr_scores))
            curr_toks, curr_scores = [], []

    if curr_toks:
        sentences.append((curr_toks, curr_scores))

    return sentences


def html_color_sentence(tokens, scores, color):
    """
    í•˜ë‚˜ì˜ ë¬¸ì¥ì„ HTML ì»¬ëŸ¬ spanìœ¼ë¡œ ë³€í™˜
    """
    html = ""
    for t, s in zip(tokens, scores):
        t_clean = clean_token(t)
        html += f"<span style='background-color:{color}; padding:2px; margin:1px; border-radius:4px;'>{t_clean}</span> "
    return html


def visualize_sentence_html_interactive(tokens, scores, sep_idx, top_k=3):
    scores = np.array(scores)

    #---------------------------
    # 1. Context / Question ë¶„ë¦¬
    #---------------------------
    context_tokens = tokens[:sep_idx]
    context_scores = scores[:sep_idx]

    question_tokens = tokens[sep_idx:]
    question_scores = scores[sep_idx:]

    #---------------------------
    # 2. Context ë¬¸ì¥ ë‹¨ìœ„
    #---------------------------
    sentences = split_tokens_to_sentences(context_tokens, context_scores)
    sentence_scores = np.array([sum(s[1]) for s in sentences])

    # Top-K ë¬¸ì¥ ì„ íƒ
    if len(sentence_scores) > 0:
        top_indices = np.argsort(sentence_scores)[-top_k:]
    else:
        top_indices = []

    # Normalize for colormap
    if len(sentence_scores) > 0:
        norm = (sentence_scores - sentence_scores.min()) / (sentence_scores.max() - sentence_scores.min() + 1e-8)
    else:
        norm = []

    cmap = plt.cm.get_cmap("coolwarm")

    #---------------------------
    # 3. HTML ìƒì„±
    #---------------------------
    html_output = "<h3>Sentence-level IG Highlight (Context Top-{}</h3>".format(top_k)
    html_output += "<div style='font-size: 16px; line-height: 2;'>"

    # Context ì¶œë ¥
    i=0
    html_output += "<h4>Context (Top Sentences Highlighted)</h4>"
    for idx, (sent_tokens, sent_scores) in enumerate(sentences):
        if idx in top_indices and i<3:
            i = i+1
            # ì¤‘ìš” ë¬¸ì¥ â†’ ì»¬ëŸ¬ë§µ ê¸°ë°˜ ê°•ì¡°
            rgba = cmap(norm[idx])
            color = f"rgba({int(rgba[0]*255)}, {int(rgba[1]*255)}, {int(rgba[2]*255)}, 0.6)"
        else:
            # ë¹„ì¤‘ìš” ë¬¸ì¥ â†’ íšŒìƒ‰
            color = "rgba(200,200,200,0.3)"

        if idx in top_indices:
            html_output += html_color_sentence(sent_tokens, sent_scores, color)
            html_output += "<br><br>"

    # Question ì¶œë ¥
    html_output += "<h4>Question</h4>"
    for t, s in zip(question_tokens, question_scores):
        t_clean = clean_token(t)
        html_output += f"<span style='background-color:rgba(255,150,150,0.7); padding:2px; margin:1px; border-radius:4px;'>{t_clean}</span> "
    html_output += "</div>"

    display(HTML(html_output))

    #---------------------------
    # 4. ê·¸ë˜í”„: Top-K ë¬¸ì¥ë§Œ í‘œì‹œ
    #---------------------------
    plt.figure(figsize=(18, 5))

    x_pos = 0
    xticks = []
    xlabels = []

    for idx in top_indices:
        sent_tokens, sent_scores = sentences[idx]
        clean_tokens = []
        for t in sent_tokens:
            clean_tokens.append(clean_token(t))
        xs = np.arange(x_pos, x_pos + len(clean_tokens))

        rgba = cmap(norm[idx])
        color = (rgba[0], rgba[1], rgba[2], 0.8)

        plt.bar(xs, sent_scores, color=color)

        xticks.extend(xs)
        xlabels.extend(clean_tokens)
        x_pos += len(sent_tokens)

    plt.title(f"Top {top_k} Context Sentences (Token-Level IG Scores)")
    plt.xticks(xticks, xlabels, rotation=90)
    plt.xlabel("Tokens")
    plt.ylabel("Attribution Score")
    plt.tight_layout()
    plt.show()



# ì˜ˆì‹œ ì‹¤í–‰
# tokens, scores, cds, raw_attn = get_phi3_token_importance(context, question, model, tokenizer)
sep_idx = next(i for i, t in enumerate(best_viz_candidate['tok_f']) if "Question" in t)
visualize_sentence_html_interactive(best_viz_candidate['tok_f'], best_viz_candidate['score_f'], sep_idx, top_k=5)

import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
with open('/content/drive/MyDrive/ColabNotebooks/datasets/phase1_experiment_results.csv', 'r') as f:
    df = pd.read_csv(f)

plt.figure(figsize=(10, 6))
sns.kdeplot(df['cds_faithful'], label='CDS Faithful', fill=True, color='blue', alpha=0.5)
sns.kdeplot(df['cds_distracted'], label='CDS Distracted', fill=True, color='orange', alpha=0.5)
plt.title('Comparison of CDS Faithful and CDS Distracted Distributions')
plt.xlabel('Score')
plt.ylabel('Density')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()
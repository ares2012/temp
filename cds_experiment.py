# -*- coding: utf-8 -*-
"""cds_experiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/ares2012/temp/blob/master/cds_experiment.ipynb

# ENV

<a target="_blank" href="https://colab.research.google.com/github/ares2012/temp/blob/master/Open_in_Colab.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

https://openincolab.com

##### ÏÑ∏ÏÖò Îã§Ïãú ÏãúÏûë(0)
"""

! pip install captum bitsandbytes llmlingua

"""##### ÎìúÎùºÏù¥Î∏å(0)"""

from google.colab import drive
drive.mount('/content/drive')

"""##### API_KEY"""

# cp -r /content/drive/MyDrive/Colab\ Notebooks/my_list.txt /content

import os
import json

file_path = '/content/my_list.txt'
file_path = '/content/drive/MyDrive/ColabNotebooks/env/my_list.txt'

with open(file_path, 'r') as f:
  keys = json.load(f)

oKEY = keys['OPENAI_API_KEY']
pKEY = keys['PINECONE_API_KEY']
hKEY = keys['huggingface.co']
hf_KEY = keys['HF_TOKEN']

from huggingface_hub import login

print(hf_KEY); login(hf_KEY)

"""##### Ï∂îÍ∞Ä ÏÑ§Ïπò
Successfully installed transformers-4.57.3


* pip install captum bitsandbytes llmlingua
* pip install --upgrade transformers


"""

# !pip install openai --upgrade --quiet

# pip install pycuda

# pip install --upgrade transformers

pip list | grep transformers

# pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers

pip uninstall -y transformers && pip install transformers==4.49.0

"""# CDS

##### Î™®Îç∏ Î°úÎìú(1)
"""

import os
import sys
import json

import torch

import numpy as np
import pandas as pd
from scipy import stats

import matplotlib.pyplot as plt
import seaborn as sns

import random
from tqdm import tqdm

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import AutoConfig, BitsAndBytesConfig
from captum.attr import LayerIntegratedGradients, IntegratedGradients

os.environ["PYTORCH_CUDA_ALLOC_CON"] = "expandable_segments:True"
#os.environ["PYTORCH_NO_CUDA_MEMORY_CACHING"] = "1"
#os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# ==========================================
# 0-2. ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è Ïó£ÏßÄ Î™®Îç∏ Î°úÎìú Î°úÏª¨ (1Î∂Ñ)
# ==========================================
torch.cuda.empty_cache()
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

##model_name = "microsoft/Phi-3-mini-4k-instruct" #(4Î∂Ñ, 52Ï¥à)
model_name = "microsoft/Phi-3-mini-128k-instruct" #(1Î∂Ñ, 57Ï¥à/3.6G)
#model_name = "microsoft/Phi-3.5-mini-instruct" #(1Î∂Ñ, 57Ï¥à/3.6G)
#model_name = "microsoft/Phi-4-mini-instruct" #(3Î∂Ñ/2.9G) #transformers==4.49.0

#model_name = "Qwen/Qwen3-0.6B" #Instruct (25Ï¥à/1.3G, 6Ï¥à/3.5G)
#model_name = "Qwen/Qwen3-1.7B" #Instruct (25Ï¥à/1.3G, 6Ï¥à/3.5G)
#model_name = "Qwen/Qwen3-4B" #Instruct (3Î∂Ñ/3.9G)
#model_name = "Qwen/Qwen3-4B-Instruct-2507" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)

#model_name = "google/gemma-3-1b-it" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)
#model_name = "google/gemma-3-4b-it" #(25Ï¥à/1.3G, 2Î∂Ñ/4.2G)
#model_name = "meta-llama/Llama-3.2-1B-Instruct" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)
model_name = "meta-llama/Llama-3.2-3B-Instruct" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)
#model_name = "LGAI-EXAONE/EXAONE-4.0-1.2B" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)
local_path = '/content/drive/MyDrive/ColabNotebooks/hub/'

# Construct the path to the model's base directory in the hub
model_base_dir = os.path.join(local_path, "models--" + model_name.replace("/", "--"))

# Construct the path to the snapshots directory
snapshots_dir = os.path.join(model_base_dir, "snapshots")

# Dynamically get the snapshot hash (assuming there's only one subdirectory in snapshots)
if os.path.exists(snapshots_dir):
    snapshot_hashes = os.listdir(snapshots_dir)
    if snapshot_hashes:
        snapshot_hash = snapshot_hashes[0] # Take the first one found
        # Construct the full local path to the model directory including the dynamically found snapshot hash
        local_model_dir = os.path.join(snapshots_dir, snapshot_hash)
    else:
        raise FileNotFoundError(f"No snapshot hashes found in {snapshots_dir}")
else:
    raise FileNotFoundError(f"Snapshots directory not found: {snapshots_dir}")
print(f"Loading model from: {local_model_dir}")

# Explicitly load the configuration first
config = AutoConfig.from_pretrained(local_model_dir, trust_remote_code=True, local_files_only=True, output_attentions=True)
print(f"Type of config after loading: {type(config)}") # Debugging line

tokenizer = AutoTokenizer.from_pretrained(
    # model_name,
    local_model_dir, # Use the direct local path
    config=config, # Pass the loaded config
    #trust_remote_code=True
)
print(f"tokenizer.vocab_files_names: {tokenizer.vocab_files_names}")

bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
)
model = AutoModelForCausalLM.from_pretrained(
        # model_name,
        local_model_dir, # Use the direct local path
        config=config, # Pass the loaded config
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        #attn_implementation="eager",
        #output_attentions=True,
        local_files_only = True # Ensure local files are used
)
# This globally disables the problematic caching mechanism.
model.config.use_cache = False

"""##### Î™®Îç∏ Îã§Ïö¥Î°úÎìú
: ÌõÑ ÎìúÎùºÏù¥Î∏å Î≥µÏÇ¨


*   cp -r /root/.cache/huggingface/hub/models--LGAI-EXAONE--EXAONE-4.0-1.2B/. /content/drive/MyDrive/ColabNotebooks/hub/models--LGAI-EXAONE--EXAONE-4.0-1.2B/




*   cp /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/072cb7562cb8c4adf682a8e186aaafa49469eb5d/configuration_phi3.py /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/f39ac1d28e925b323eae81227eaba4464caced4e
*   cp /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/f39ac1d28e925b323eae81227eaba4464caced4e




"""

! cp -r /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/. /content/drive/MyDrive/ColabNotebooks/hub/models--meta-llama--Llama-3.2-3B-Instruct/

#configuration_phi3.py, modeling_phi3.py

!cp /root/.cache/huggingface/hub/models--microsoft--Phi-4-mini-instruct/snapshots/cfbefacb99257ffa30c83adab238a50856ac3083/modeling_phi3.py /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-4-mini-instruct/snapshots/cfbefacb99257ffa30c83adab238a50856ac3083

# ==========================================
# 0-1. ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è Ïó£ÏßÄ Î™®Îç∏ Îã§Ïö¥ Î°úÎìú (1Î∂Ñ)
# ==========================================
torch.cuda.empty_cache()
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

##model_name = "microsoft/Phi-3-mini-4k-instruct" #(4Î∂Ñ, 52Ï¥à/3.6G)
#model_name = "microsoft/Phi-3-mini-128k-instruct" #(4Î∂Ñ, 57Ï¥à/3.6G)
#model_name = "microsoft/Phi-3.5-mini-instruct" #(1Î∂Ñ/7.64G, 52Ï¥à/3.6G)
model_name = "microsoft/Phi-4-mini-instruct" #(1Î∂Ñ/7.64G, 52Ï¥à/3.6G)

#model_name = "Qwen/Qwen3-0.6B" #Instruct (25Ï¥à/1.3G, 6Ï¥à/3.5G)
#model_name = "Qwen/Qwen3-1.7B" #(43Ï¥à/4.1G, 6Ï¥à/3.5G)
#model_name = "Qwen/Qwen3-4B" #(1Î∂Ñ/8.1G, 6Ï¥à/3.5G)
##model_name = "Qwen/Qwen3-0.6B-Base" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)
##model_name = "Qwen/Qwen3-4B-Base" #(1Î∂Ñ/8.1G, 6Ï¥à/3.5G)
##model_name = "Qwen/Qwen3-4B-Instruct-2507" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)

#model_name = "LGAI-EXAONE/EXAONE-4.0-1.2B" #(40Ï¥à/2.56G, 6Ï¥à/1.3G)

#model_name = "google/gemma-3-1b-it" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)
#model_name = "google/gemma-3-4b-it" #(25Ï¥à/1.3G, 6Ï¥à/4.6G)
#model_name = "meta-llama/Llama-3.2-1B" #(25Ï¥à/1.3G, 6Ï¥à/0.3G)
#model_name = "meta-llama/Llama-3.2-3B" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)
#model_name = "meta-llama/Llama-3.2-1B-Instruct" #(25Ï¥à/1.3G, 6Ï¥à/0.3G)
#model_name = "meta-llama/Llama-3.2-3B-Instruct" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)


tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    #trust_remote_code=True
)
print(f"tokenizer.vocab_files_names: {tokenizer.vocab_files_names}")

bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
)
model = AutoModelForCausalLM.from_pretrained(
        model_name,
        #quantization_config=bnb_config,
        device_map="auto",
        #torch_dtype="bfloat16"
        torch_dtype="auto",
        trust_remote_code=True,
        attn_implementation="eager"
)
# This globally disables the problematic caching mechanism.
model.config.use_cache = False

"""##### ÌîÑÎ°¨ÌîÑÌä∏


* apply chat model
* context+question


"""

# ==========================================
# 1. XAI: IG Î∂ÑÏÑù Î∞è CDS Í≥ÑÏÇ∞
# ==========================================
from captum.attr import IntegratedGradients
#from torch.cuda import AcceleratorError
#import pycuda.driver as cuda
def get_phi3_token_importance(context, question, model, tokenizer):
    # Î™ÖÏãúÏ†ÅÏù∏ Íµ¨Ï°∞Î•º Í∞ÄÏßÑ ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± (Separator Í∞êÏßÄ Ïö©Ïù¥)
    # Phi-3 ÌÖúÌîåÎ¶ø + Î™ÖÌôïÌïú Íµ¨Î∂ÑÏûê
    full_text = f"<|user|>\nContext:\n{context}\n\nQuestion:\n{question}\n\nAnswer based on the context:<|end|>\n<|assistant|>"

    # 0. ÏûÖÎ†• ID Ï∂îÏ∂ú
    torch.cuda.empty_cache()
    inputs = tokenizer(full_text, return_tensors="pt", add_special_tokens=False).to(DEVICE)
    input_ids = inputs.input_ids
    torch.cuda.empty_cache()

    # 1. Raw Attention Ï∂îÏ∂ú (Forward Pass)
    with torch.no_grad():
      #output = model(**inputs)#; print(type(output))
      output = model(input_ids, output_attentions=True) #  AttentionÎßå Ï∂îÏ∂ú
      # ÎßàÏßÄÎßâ Î†àÏù¥Ïñ¥Ïùò Ïñ¥ÌÖêÏÖò (Batch, Head, Seq, Seq) -> Head ÌèâÍ∑† -> (Seq, Seq)
      raw_attn = output.attentions[-1][0].mean(dim=0).cpu().numpy()
      #raw_attn = output.attentions[0][0].mean(dim=0).cpu().numpy()

    torch.cuda.empty_cache()
    # 2. ÏûÑÎ≤†Îî© Î†àÏù¥Ïñ¥ Ï∞æÍ∏∞
    if hasattr(model, "model") and hasattr(model.model, "embed_tokens"):
        embedding_layer = model.model.embed_tokens
    elif hasattr(model, "embed_tokens"): # Llama/Gemma ÏùºÎ∂Ä
        embedding_layer = model.embed_tokens
    else:
        embedding_layer = model.get_input_embeddings()

    torch.cuda.empty_cache()
    # 3. ÏûÑÎ≤†Îî© Î≤°ÌÑ∞Î•º ÎØ∏Î¶¨ Í≥ÑÏÇ∞ > CaptumÏùò Input
    # (4-bit Î™®Îç∏ Îì±ÏóêÏÑú Gradient Í≥ÑÏÇ∞ÏùÑ ÏúÑÌï¥ requires_grad ÏÑ§Ï†ïÏù¥ ÌïÑÏöîÌï† Ïàò ÏûàÏùå)
    input_embeddings = embedding_layer(input_ids)

    torch.cuda.empty_cache()
    # 4. Forward Ìï®Ïàò Ï†ïÏùò (ÏûÑÎ≤†Îî©ÏùÑ ÏûÖÎ†•ÏúºÎ°ú Î∞õÏùå)
    def forward_func(inputs_embeds):
        # inputs_embedsÎ•º Î™®Îç∏Ïóê ÏßÅÏ†ë Ï£ºÏûÖ
        #outputs = model(inputs_embeds=inputs_embeds)
        outputs = model(inputs_embeds=inputs_embeds, output_attentions=False) # attention ÎπÑÌôúÏÑ±Ìôî
        #return outputs.logits[0, -1, :].max().unsqueeze(0)
        return outputs.logits[0, -1].max().unsqueeze(0)

    try:
        # 5. [Î≥ÄÍ≤Ω] LayerIntegratedGradients -> IntegratedGradients
        # Î†àÏù¥Ïñ¥Í∞Ä ÏïÑÎãàÎùº 'ÏûÖÎ†• ÌÖêÏÑú(input_embeddings)' ÏûêÏ≤¥Î•º Î∂ÑÏÑù ÎåÄÏÉÅÏúºÎ°ú Ìï®
        ig = IntegratedGradients(forward_func)

        torch.cuda.empty_cache()
        # 6. ÏÜçÏÑ±(Attribute) Í≥ÑÏÇ∞
        attributions, delta = ig.attribute(
            inputs=input_embeddings,
            # BaselineÏùÄ 0 Î≤°ÌÑ∞ (All-zero embeddings)
            baselines=torch.zeros_like(input_embeddings),
            n_steps=1,
            internal_batch_size=1, # Ìïú Î≤àÏóê 1Í∞ú Îã®Í≥ÑÏî©Îßå Í≥ÑÏÇ∞ÌïòÏó¨ Î©îÎ™®Î¶¨ Ï†àÏïΩ
            return_convergence_delta=True
        )
    except RuntimeError as e:
    #except (AcceleratorError, RuntimeError) as e:
    #except (cuda.Error, MemoryError, RuntimeError) as e:
      if "out of memory" in str(e):
        #del attributions, delta, inputs, input_ids, input_embeddings
        torch.cuda.empty_cache()
        print("Skipping sample due to OOM.")
        torch.cuda.empty_cache()
        return None, None, None, None
      else:
        raise e

    # 7. Ï§ëÏöîÎèÑ Ï†êÏàò Ìï©ÏÇ∞ (L2 Norm)
    scores = torch.norm(attributions, dim=-1).squeeze().tolist()
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])

    # 8. CDS Í≥ÑÏÇ∞ (Context ÏòÅÏó≠: 'Question:' Ï†ÑÍπåÏßÄ)
    try:
        # ÌÜ†ÌÅ∞ Ï§ë 'Question' Î¨∏ÏûêÏó¥ÏùÑ Ìè¨Ìï®ÌïòÎäî Ïù∏Îç±Ïä§ Ï∞æÍ∏∞
        sep_idx = next(i for i, t in enumerate(tokens) if "Question" in t)
        context_score = sum(scores[:sep_idx])
        total_score = sum(scores)
        cds = context_score / total_score if total_score > 0 else 0
    except StopIteration:
        cds = 0.5 # Íµ¨Î∂ÑÏûê Î™ª Ï∞æÏùå
        sep_idx = len(tokens) // 2

    del attributions, delta, inputs, input_ids, input_embeddings, output
    torch.cuda.empty_cache()
    #print(tokens, scores, cds, raw_attn)

    return tokens, scores, cds, raw_attn

"""##### CDS/ATT ÏÇ∞Ï∂ú(2)
: Cuda/gpu ÏµúÏ†ÅÌôî

import gc
gc.collect(); torch.cuda.empty_cache()


torch.cuda.memory_reserved(0), torch.cuda.memory_allocated(0), torch.cuda.get_device_properties(0).total_memory

torch.cuda.mem_get_info(), torch.cuda.mem_get_info()[0]
"""

import torch
from captum.attr import IntegratedGradients

MAX_CHUNK_TOKENS = 1024   # ÎÑàÎ¨¥ ÌÅ¨Î©¥ IGÏóêÏÑú OOM ‚Üí chunk Îã®ÏúÑÎ°ú Ï≤òÎ¶¨
MAX_CHUNK_TOKENS = 512
DEVICE = "cuda"

def chunk_tokens(input_ids, chunk_size=MAX_CHUNK_TOKENS):
    chunks = []
    for i in range(0, len(input_ids), chunk_size):
        chunks.append(input_ids[i:i + chunk_size])
    return chunks

def get_phi3_token_importance(context, question, model, tokenizer):
    """
    Í∏∏Ïù¥ 5000 Ïù¥ÏÉÅÏùò Î¨∏ÏÑúÏóêÏÑúÎèÑ OOMÏù¥ Î∞úÏÉùÌïòÏßÄ ÏïäÎèÑÎ°ù
    ÏûÖÎ†•ÏùÑ Chunk Îã®ÏúÑÎ°ú ÏûòÎùº IG Í≥ÑÏÇ∞ÏùÑ ÏàòÌñâÌïòÎäî Î≤ÑÏ†Ñ.
    """

    full_text = (
        "<|user|>\nContext:\n" +
        context +
        "\n\nQuestion:\n" +
        question +
        "\n\nAnswer based on the context:<|end|>\n<|assistant|>"
    )

    # -------------------------------------------------------
    # 1-1. ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï (CPU Ïú†ÏßÄ)
    # -------------------------------------------------------
    torch.cuda.empty_cache()
    inputs = tokenizer(full_text, return_tensors="pt", add_special_tokens=False)
    input_ids = inputs.input_ids[0].tolist()  # list Î°ú Î≥ÄÌôòÌïòÏó¨ CPU Ïú†ÏßÄ

    # Îß§Ïö∞ Í∏¥ Í≤ΩÏö∞ chunkÎ°ú Î∂ÑÎ¶¨
    token_chunks = chunk_tokens(input_ids, MAX_CHUNK_TOKENS)

    # Ï†ÑÏ≤¥ Í≤∞Í≥º Ï†ÄÏû•Ïö©
    all_tokens = []
    all_scores = []
    all_raw_attn = []

    # -------------------------------------------------------
    # 1-2. Attention Ï∂îÏ∂ú (Chunk Îã®ÏúÑ)
    # -------------------------------------------------------
    for c_idx, chunk in enumerate(token_chunks):
        input_tensor = torch.tensor(chunk, dtype=torch.long).unsqueeze(0).to(DEVICE)

        with torch.no_grad():
            out = model(input_tensor, output_attentions=True)
            raw_attn = out.attentions[-1][0].mean(dim=0).cpu().numpy()
            all_raw_attn.append(raw_attn)

        # GPU Î©îÎ™®Î¶¨ Ï¶âÏãú ÎπÑÏö∞Í∏∞
        del out, input_tensor
        torch.cuda.empty_cache()

    # -------------------------------------------------------
    # 1-3. ÏûÑÎ≤†Îî© Î†àÏù¥Ïñ¥ Í∞ÄÏ†∏Ïò§Í∏∞
    # -------------------------------------------------------
    if hasattr(model, "model") and hasattr(model.model, "embed_tokens"):
        embedding_layer = model.model.embed_tokens
    elif hasattr(model, "embed_tokens"):
        embedding_layer = model.embed_tokens
    else:
        embedding_layer = model.get_input_embeddings()

    # -------------------------------------------------------
    # 1-4. Chunk Îã®ÏúÑ IG Í≥ÑÏÇ∞
    # -------------------------------------------------------
    for c_idx, chunk in enumerate(token_chunks):

        # ÌÜ†ÌÅ∞ ‚Üí ÌÖêÏÑú
        ids_tensor = torch.tensor(chunk, dtype=torch.long).unsqueeze(0).to(DEVICE)

        # ÏûÑÎ≤†Îî© Í≥ÑÏÇ∞
        input_embeds = embedding_layer(ids_tensor)

        # Forward Ìï®Ïàò (embedding Í∏∞Î∞ò)
        def forward_func(embeds):
            out = model(inputs_embeds=embeds, output_attentions=False)
            return out.logits[0, -1].max().unsqueeze(0)

        ig = IntegratedGradients(forward_func)

        try:
            attributions, delta = ig.attribute(
                inputs=input_embeds,
                baselines=torch.zeros_like(input_embeds),
                n_steps=1,
                internal_batch_size=1,
                return_convergence_delta=True
            )
        except RuntimeError as e:
            if "out of memory" in str(e):
                torch.cuda.empty_cache()
                print("Chunk", c_idx, "IG skipped due to OOM")
                continue
            else:
                raise e

        # L2 Norm Í∏∞Î∞ò Ï§ëÏöîÎèÑ Í≥ÑÏÇ∞
        scores = torch.norm(attributions, dim=-1).squeeze().tolist()
        tokens = tokenizer.convert_ids_to_tokens(chunk)
        token_attr = attributions.sum(dim=-1).squeeze(0).abs()

        all_tokens.extend(tokens)
        if not isinstance(scores, list):
            all_scores.append(scores)
        else:
            all_scores.extend(scores)

        del attributions, delta, ids_tensor, input_embeds
        torch.cuda.empty_cache()

    # -------------------------------------------------------
    # 1-5. CDS Í≥ÑÏÇ∞
    # -------------------------------------------------------
    try:
        sep_idx = next(i for i, t in enumerate(all_tokens) if "Question" in t)
        context_score = sum(all_scores[:sep_idx])
        total_score = sum(all_scores)
        cds = context_score / total_score if total_score > 0 else 0
    except StopIteration:
        cds = 0.5
        sep_idx = len(all_tokens) // 2

    return all_tokens, all_scores, cds, all_raw_attn, token_attr, sep_idx, inputs.input_ids[0]

# -------------------------------------------------------
# 1-6. Attribute Shift Í≥ÑÏÇ∞
# -------------------------------------------------------
def compute_shift(attr_f, attr_d, ctx_mask, q_mask):
    A_ctx_f = attr_f[ctx_mask].sum()
    A_ctx_d = attr_d[ctx_mask].sum()
    A_q_f = attr_f[q_mask].sum()
    A_q_d = attr_d[q_mask].sum()

    context_drop = (A_ctx_f - A_ctx_d).item()
    question_rise = (A_q_d - A_q_f).item()
    total_shift = context_drop + question_rise

    return {
        "context_drop": context_drop,
        "question_rise": question_rise,
        "total_shift": total_shift
    }


def build_masks(input_ids, context_start, context_end):
    # ÏßàÎ¨∏ ÏòÅÏó≠ = [1 : context_start)
    q_mask = torch.zeros_like(input_ids, dtype=torch.bool)
    q_mask[1:context_start] = True

    # Î¨∏Îß• ÏòÅÏó≠
    ctx_mask = torch.zeros_like(input_ids, dtype=torch.bool)
    ctx_mask[context_start:context_end] = True

    return q_mask, ctx_mask

def build_masks(input_ids, sep_idx):
    # ÏßàÎ¨∏ ÏòÅÏó≠ = [ sep_idx : ]
    q_mask = torch.zeros_like(input_ids, dtype=torch.bool)
    q_mask[sep_idx:] = True

    # Î¨∏Îß• ÏòÅÏó≠ = [ : sep_idx ]
    ctx_mask = torch.zeros_like(input_ids, dtype=torch.bool)
    ctx_mask[:sep_idx] = True

    return q_mask, ctx_mask

def find_context_question_spans(token_list):
    """
    Qwen/Phi Î™®Îç∏Ïö©
    ÌÜ†ÌÅ∞ÏóêÏÑú 'Context:' / 'Question:' Î¨∏ÏûêÏó¥ Ìå®ÌÑ¥ÏúºÎ°ú ÏúÑÏπò ÌÉêÏÉâ
    """

    context_key = "Context"
    question_key = "Question"

    # Î∂ÄÎ∂Ñ Î¨∏ÏûêÏó¥ Îß§Ïπ≠ÏúºÎ°ú robust Ï≤òÎ¶¨
    ctx_idx = next((i for i, t in enumerate(token_list) if context_key in t), None)
    q_idx = next((i for i, t in enumerate(token_list) if question_key in t), None)

    if ctx_idx is None or q_idx is None:
        # fallback: Ï†àÎ∞òÏúºÎ°ú Î∂ÑÎ¶¨
        mid = len(token_list) // 2
        return (1, mid), (mid, len(token_list))

    # Ïã§Ï†ú contextÎäî "Context:" Îã§Ïùå Ï§ÑÎ∂ÄÌÑ∞ ÏãúÏûë
    # Ïòà: ["Context", ":</n>", "hello", "world", ...]
    context_start = ctx_idx + 1
    question_start = q_idx + 1

    context_span = (context_start, question_start)
    question_span = (question_start, len(token_list))

    return context_span, question_span


def compute_attribution_shift(tokens_f, scores_f, tokens_d, scores_d):
    """
    Faithful / Distracted Attribution Shift Í≥ÑÏÇ∞
    """

    # 1) Îëê ÏºÄÏù¥Ïä§ÏóêÏÑú context/question Íµ¨Í∞Ñ Ï∞æÍ∏∞
    ctx_f, q_f = find_context_question_spans(tokens_f)
    ctx_d, q_d = find_context_question_spans(tokens_d)

    c_s_f, c_e_f = ctx_f
    q_s_f, q_e_f = q_f

    c_s_d, c_e_d = ctx_d
    q_s_d, q_e_d = q_d

    # 2) Attribution Ìï©ÏÇ∞
    A_ctx_f = sum(scores_f[c_s_f:c_e_f])
    A_ctx_d = sum(scores_d[c_s_d:c_e_d])

    A_q_f = sum(scores_f[q_s_f:q_e_f])
    A_q_d = sum(scores_d[q_s_d:q_e_d])

    # 3) Shift Í≥ÑÏÇ∞
    context_drop = A_ctx_f - A_ctx_d
    question_rise = A_q_d - A_q_f
    total_shift = context_drop + question_rise

    return {
        "context_drop": context_drop,
        "question_rise": question_rise,
        "total_shift": total_shift,
        "ctx_span_f": ctx_f,
        "ctx_span_d": ctx_d,
        "q_span_f": q_f,
        "q_span_d": q_d,
    }

"""##### Îç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú
 : ÌõÑ ÎìúÎùºÏù¥Î∏å Î≥µÏÇ¨


*   cp -r /root/.cache/huggingface/hub/datasets--ms_marco/. /content/drive/MyDrive/ColabNotebooks/hub/datasets--ms_marco/
*   cp -r /root/.cache/huggingface/datasets/ms_marco/. /content/drive/MyDrive/ColabNotebooks/datasets/ms_marco/






"""

# ==========================================
# 2. Î©îÏù∏ Ïã§Ìóò Î£®ÌîÑ (Îç∞Ïù¥ÌÑ∞ ÏàòÏßë) 3Ï¥à
# ==========================================

#dataset = load_dataset('hotpot_qa', 'distractor', split="validation", cache_dir="/content/drive/MyDrive/ColabNotebooks/datasets")
#dataset1 = load_dataset('squad', 'plain_text', split="validation", cache_dir="/content/drive/MyDrive/ColabNotebooks/datasets")
#dataset2 = load_dataset('ms_marco', 'v2.1', split="validation", cache_dir="/content/drive/MyDrive/ColabNotebooks/datasets")
#dataset2 = dataset2.rename_column("query", "question").rename_column("passages", "context")
dataset3 = load_dataset('hotpot_qa', 'distractor', split="validation", cache_dir="/content/drive/MyDrive/ColabNotebooks/hub")

#dataset1 #dataset2 #
dataset3

results = []; NUM_SAMPLES = 200  # ÎÖºÎ¨∏Ïö©ÏúºÎ°úÎäî 100~200Í∞ú Í∂åÏû•(10Í∞ú/1Î∂Ñ)
dataset = dataset3.shuffle(seed=42).select(range(NUM_SAMPLES))

best_viz_candidate = None
max_cds_diff = -1
print(f"\nüöÄ Collecting Data from {NUM_SAMPLES} samples...")

for i in tqdm(range(NUM_SAMPLES)):
    torch.cuda.empty_cache()#; print(torch.cuda.memory_allocated(0))
    data = dataset[i]
    question = data['question']
    # [Ï°∞Í±¥ 1] Faithful: Ïò¨Î∞îÎ•∏ Î¨∏Îß•
    cds_faithful = " ".join(["".join(sent) for sent in data['context']['sentences']])
    #cds_faithful = " ".join(["".join(sent) for sent in data['context']['passage_text']])
    #cds_faithful= data['context']
    #print("cds_faithful:",cds_faithful)
    # [Ï°∞Í±¥ 2] Distracted: Î¨¥Í¥ÄÌïú Î¨∏Îß• (ÎûúÎç§ ÏÉòÌîåÎßÅ)
    rand_idx = random.randint(0, len(dataset)-1)
    while rand_idx == i: rand_idx = random.randint(0, len(dataset)-1)
    cds_distracted = " ".join(["".join(sent) for sent in dataset[rand_idx]['context']['sentences']])
    #cds_distracted = " ".join(["".join(sent) for sent in dataset[rand_idx]['context']['passage_text']])
    #cds_distracted = dataset[rand_idx]['context']
    #print("cds_distracted:",cds_distracted)
    print(i, len(cds_faithful), len(cds_distracted))

    # Î∂ÑÏÑù Ïã§Ìñâ
    tok_f, score_f, cds_f, attn_f = get_phi3_token_importance(cds_faithful, question, model, tokenizer)
    tok_h, score_h, cds_h, attn_h = get_phi3_token_importance(cds_distracted, question, model, tokenizer)
    if cds_f is None or cds_h is None: continue # ÏóêÎü¨/OOM Ïä§ÌÇµ

    # Í≤∞Í≥º Ï†ÄÏû•
    diff = cds_f - cds_h
    results.append({"id": i,"cds_faithful": cds_f,"len_faithful": len(cds_faithful),
                    "cds_distracted": cds_h, "len_distracted": len(cds_distracted), "diff": diff })
    # ÏãúÍ∞ÅÌôîÏö© 'ÏµúÍ≥†Ïùò ÏÉòÌîå' Ï†ÄÏû• (Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• ÌÅ∞ Í≤É)
    if diff > max_cds_diff:
      max_cds_diff = diff
      best_viz_candidate = {
          "tok_f": tok_f, "score_f": score_f, "attn_f": attn_f,
          "tok_h": tok_h, "score_h": score_h, "attn_h": attn_h,
          "q": question, "f": cds_faithful, "d": cds_distracted         }

import tqdm
import random
import pandas as pd
from datasets import load_dataset

NUM_SAMPLES = 200
data_path = "/content/drive/MyDrive/ColabNotebooks/datasets"
test_data = [['squad', 'plain_text'],['ms_marco', 'v2.1'],['hotpot_qa', 'distractor']]
for n in test_data:
  dataset = load_dataset(n[0], n[1], split="validation", cache_dir=data_path)
  if n[0] == 'ms_marco':
    dataset = dataset.rename_column("query", "question").rename_column("passages", "context")
  print(dataset)

  dataset = dataset.shuffle(seed=42).select(range(NUM_SAMPLES))
  print(f"\nüöÄ Collecting Data from {NUM_SAMPLES} samples...")
  for i in tqdm.tqdm(range(NUM_SAMPLES)):
      data = dataset[i]
      question = data['question']
      # [Ï°∞Í±¥ 1] Faithful: Ïò¨Î∞îÎ•∏ Î¨∏Îß•
      if n[0] == 'hotpot_qa':
        cds_faithful = " ".join(["".join(sent) for sent in data['context']['sentences']])
      elif n[0] == 'ms_marco':
        cds_faithful = " ".join(["".join(sent) for sent in data['context']['passage_text']])
      else:
        cds_faithful= data['context']
      #print("cds_faithful:",cds_faithful)
      # [Ï°∞Í±¥ 2] Distracted: Î¨¥Í¥ÄÌïú Î¨∏Îß• (ÎûúÎç§ ÏÉòÌîåÎßÅ)
      rand_idx = random.randint(0, len(dataset)-1)
      while rand_idx == i: rand_idx = random.randint(0, len(dataset)-1)
      if n[0] == 'hotpot_qa':
        cds_distracted = " ".join(["".join(sent) for sent in dataset[rand_idx]['context']['sentences']])
      elif n[0] == 'ms_marco':
        cds_distracted = " ".join(["".join(sent) for sent in dataset[rand_idx]['context']['passage_text']])
      else:
        cds_distracted = dataset[rand_idx]['context']
      #print("cds_distracted:",cds_distracted)
      print(i, len(cds_faithful), len(cds_distracted))

      context = cds_faithful
      context_start = 0
      context_end = len(context)
      full_text = (
          "<|user|>\nContext:\n" +
          context +
          "\n\nQuestion:\n" +
          question +
          "\n\nAnswer based on the context:<|end|>\n<|assistant|>"
      )

      all_tokens = full_text.split()
      try:
          sep_idx = next(i for i, t in enumerate(all_tokens) if "Question" in t)
          print(all_tokens[sep_idx:])
          #context_score = sum(all_scores[:sep_idx])
          #total_score = sum(all_scores)
          #cds = context_score / total_score if total_score > 0 else 0
          #print(context_score, total_score, cds)
      except StopIteration:
          cds = 0.5
          sep_idx = len(all_tokens) // 2

"""##### Ïã§Ìóò(3)
: Îç∞Ïù¥ÌÑ∞ÏÖã+Í≤∞Í≥ºÏ†ÄÏû•

*   dataset3[0]['context']['sentences'][0]
*   dataset2[0]['context']['passage_text']
*   dataset1[0]['context']
"""

def save_results(results, best_viz_candidate, data_name):
    rst_path = "/content/drive/MyDrive/ColabNotebooks/results/CDS_4B/"
    csv_name = rst_path + model_name.split('/')[1]+"_" +data_name+".csv"
    df = pd.DataFrame(results)
    df.to_csv(csv_name, index=False)
    print(f"‚úÖ Phase1 {model_name}_{data_name} Experiment results Saved.",)


    # Prepare a copy of the dictionary to modify for JSON serialization
    # This prevents modifying the original best_viz_candidate if it's used elsewhere as numpy array
    serializable_candidate = best_viz_candidate.copy()
    serializable_candidate['attn_f'] = best_viz_candidate['attn_f'][0].tolist()
    serializable_candidate['attn_d'] = best_viz_candidate['attn_d'][0].tolist()

    json_name = rst_path + model_name.split("/")[1]+"_" +data_name+".json"
    with open(json_name, 'w') as f:
        json.dump(serializable_candidate, f)

# ==========================================
# 2. Î©îÏù∏ Ïã§Ìóò Î£®ÌîÑ (Îç∞Ïù¥ÌÑ∞ÏÖã *3) 25Î∂Ñ, 65Î∂Ñ
# ==========================================

data_path = "/content/drive/MyDrive/ColabNotebooks/datasets"
test_data = [['squad', 'plain_text'],['ms_marco', 'v2.1'],['hotpot_qa', 'distractor']]
#test_data = [['hotpot_qa', 'distractor']]
for n in test_data:
  dataset = load_dataset(n[0], n[1], split="validation", cache_dir=data_path)
  if n[0] == 'ms_marco':
    dataset = dataset.rename_column("query", "question").rename_column("passages", "context")
  print(dataset)

  results = []; best_viz_candidate = None; max_cds_diff = -1
  NUM_SAMPLES = 200  # ÎÖºÎ¨∏Ïö©ÏúºÎ°ú 100~200Í∞ú Ï∂îÏ∂ú(10Í∞ú/1Î∂Ñ)
  dataset = dataset.shuffle(seed=42).select(range(NUM_SAMPLES))
  print(f"\nüöÄ Collecting Data from {NUM_SAMPLES} samples...")
  for i in tqdm(range(NUM_SAMPLES)):
      torch.cuda.empty_cache()#; print(torch.cuda.memory_allocated(0))
      data = dataset[i]
      question = data['question']
      # [Ï°∞Í±¥ 1] Faithful: Ïò¨Î∞îÎ•∏ Î¨∏Îß•
      if n[0] == 'hotpot_qa':
        cds_faithful = " ".join(["".join(sent) for sent in data['context']['sentences']])
      elif n[0] == 'ms_marco':
        cds_faithful = " ".join(["".join(sent) for sent in data['context']['passage_text']])
      else:
        cds_faithful= data['context']
      #print("cds_faithful:",cds_faithful)
      # [Ï°∞Í±¥ 2] Distracted: Î¨¥Í¥ÄÌïú Î¨∏Îß• (ÎûúÎç§ ÏÉòÌîåÎßÅ)
      rand_idx = random.randint(0, len(dataset)-1)
      while rand_idx == i: rand_idx = random.randint(0, len(dataset)-1)
      if n[0] == 'hotpot_qa':
        cds_distracted = " ".join(["".join(sent) for sent in dataset[rand_idx]['context']['sentences']])
      elif n[0] == 'ms_marco':
        cds_distracted = " ".join(["".join(sent) for sent in dataset[rand_idx]['context']['passage_text']])
      else:
        cds_distracted = dataset[rand_idx]['context']
      #print("cds_distracted:",cds_distracted)
      print(i, len(cds_faithful), len(cds_distracted))

      # 1) CDS Í≥ÑÏÇ∞
      tok_f, score_f, cds_f, attn_f, attr_f, sep_f, ids_f = get_phi3_token_importance(cds_faithful, question, model, tokenizer)
      tok_d, score_d, cds_d, attn_d, attr_d, sep_d, ids_d = get_phi3_token_importance(cds_distracted, question, model, tokenizer)
      if cds_f is None or cds_d is None: continue # ÏóêÎü¨/OOM Ïä§ÌÇµ

      # 2) ÎßàÏä§ÌÅ¨ Í≥ÑÏÇ∞
      q_mask_f, ctx_mask_f = build_masks(ids_f, sep_f)
      q_mask_d, ctx_mask_d = build_masks(ids_d, sep_d)

      # 3) Attribution Shift Í≥ÑÏÇ∞
      #shift = compute_shift(attr_f, attr_d, ctx_mask_f, q_mask_f)
      shift = compute_attribution_shift(tok_f, score_f, tok_d, score_d)

      # 4) Í≤∞Í≥º Ï†ÄÏû•
      diff = cds_f - cds_d; len_diff = len(cds_faithful) - len(cds_distracted)
      results.append({"id": i,"cds_faithful": cds_f, "cds_distracted": cds_d, "cds_diff": diff,
                      "len_faithful": len(cds_faithful), "len_distracted": len(cds_distracted), "len_diff": len_diff,
                      "context_drop": shift["context_drop"], "question_rise": shift["question_rise"], "total_shift": shift["total_shift"]})
      # 5) ÏãúÍ∞ÅÌôîÏö© Ï†ÄÏû• (Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• ÌÅ∞ Í≤É)
      if diff > max_cds_diff:
        max_cds_diff = diff
        best_viz_candidate = {
            "tok_f": tok_f, "score_f": score_f, "attn_f": attn_f,
            "tok_d": tok_d, "score_d": score_d, "attn_d": attn_d,
            "q": question, "f": cds_faithful, "d": cds_distracted         }

  # Í≤∞Í≥º Ï†ÄÏû•
  save_results(results, best_viz_candidate, n[0])

"""##### Í≤∞Í≥º Ï†ÄÏû•"""

df = pd.DataFrame(results)
df.shape, df.describe()

# CSV Ï†ÄÏû•
df = pd.DataFrame(results)

file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-4B_hotpot_qa_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-4B_ms_marco_v2-1_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-4B_squad_with_len.csv"

#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_hotpot_qa_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_ms_marco_v2-1_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_squad_with_len.csv"

#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_hotpot_qa_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_ms_marco_v2-1_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_squad_with_len.csv"

df.to_csv(file_name, index=False)
df.to_csv("phase1_experiment_results.csv", index=False)
print("‚úÖ Phase1 Experiment results Saved.",)

import json
import numpy as np

# Prepare a copy of the dictionary to modify for JSON serialization
# This prevents modifying the original best_viz_candidate if it's used elsewhere as numpy array
serializable_candidate = best_viz_candidate.copy()
serializable_candidate['attn_f'] = best_viz_candidate['attn_f'][0].tolist()
serializable_candidate['attn_h'] = best_viz_candidate['attn_h'][0].tolist()

file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-4B_hotpot_qa_best_results.json"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-4B_ms_marco_v2-1_best_results.json"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-4B_squad_best_results.json"

#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_hotpot_qa_best_results.json"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_ms_marco_v2-1_best_results.json"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_squad_best_results.json"

#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_hotpot_qa_best_results.json"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_ms_marco_v2-1_best_results.json"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_squad_best_results.json"

with open(file_name, 'w') as f:
    json.dump(serializable_candidate, f)

type(best_viz_candidate), best_viz_candidate.keys()

type(best_viz_candidate['score_f']), len(best_viz_candidate['score_f']), best_viz_candidate['score_f'][0]

type(best_viz_candidate['attn_f']), len(best_viz_candidate['attn_f']), best_viz_candidate['attn_f'][0], len(best_viz_candidate['attn_f'][0])

"""# Í≤∞Í≥º Î∂ÑÏÑù(4)"""

import glob

file_num = 0
rst_path = "/content/drive/MyDrive/ColabNotebooks/results/CDS_4B/*"
file_list = glob.glob(rst_path)
file_list_csv = [file for file in file_list if file.endswith(".csv")]
file_list_csv, file_list_csv[file_num].split('/')[-1].split(".")[0], len(file_list_csv)

import glob

file_num = 0
rst_path = "/content/drive/MyDrive/ColabNotebooks/results/CDS_4B/*"
file_list = glob.glob(rst_path)
file_list_json = [file for file in file_list if file.endswith(".json")]
file_list_json, file_list_json[file_num].split('/')[-1].split(".")[0]

"""##### Ï†ïÎüâ Î∂ÑÏÑù(4-1)"""

# ==========================================
# 3. Ï†ïÎüâ Î∂ÑÏÑù: ÌÜµÍ≥Ñ Í≤ÄÏ†ï (T-test & kdeplot)
# ==========================================
import numpy as np
import pandas as pd
from scipy import stats

import matplotlib.pyplot as plt
import seaborn as sns

#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_hotpot_qa_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_ms_marco_v2-1_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Qwen3-0.6B_squad_with_len.csv"

#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_hotpot_qa_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_ms_marco_v2-1_with_len.csv"
#file_name = "/content/drive/MyDrive/ColabNotebooks/results/CDS/Phi-3-mini_squad_with_len.csv"

file_num = 0
file_name = file_list_csv[file_num]; print(file_name)
with open(file_name, 'r') as f:
    df = pd.read_csv(f); print(df.shape)
    #df = df[(df['len_faithful']< 1200) & (df['len_distracted'] < 1200)];  print(df.shape)
    #df = df[(df['len_faithful']< 3500) & (df['len_distracted'] < 3500)];  print(df.shape)
    #df = df[(df['len_faithful']>= 3500) | (df['len_distracted'] >= 3500)];  print(df.shape)
    #df = df[(df['len_faithful']< 4400) & (df['len_distracted'] < 4400)];  print(df.shape)
    #df = df[(df['len_faithful'] >= 4400) & (df['len_distracted'] >= 4400)];  print(df.shape)

    #df['len_diff'] = df['len_faithful'] - df['len_distracted']
    #df = df[(df['len_diff']< 0)];  print(df.shape)

df.describe()



# Í∏∏Ïù¥ ÎπÑÍµê
t_stat, p_val = stats.ttest_rel(df['len_faithful'], df['len_distracted'])
print("\n" + "="*50)
print("üìä [Phase 1] Quantitative Results by length")
print("="*50)
print(f"Mean len (Faithful):     {df['len_faithful'].mean():.4f} (std: {df['len_faithful'].std():.4f})")
print(f"Mean len (Distracted): {df['len_distracted'].mean():.4f} (std: {df['len_distracted'].std():.4f})")
print(f"Gap (Faithful - Distracted): {df['len_diff'].mean():.4f}")
print("-" * 50)
print(f"Statistical Significance (Paired T-test):")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value:     {p_val:.4e}")
if p_val < 0.05:
    print("‚úÖ Result: Statistically Significant (p < 0.05)")
else:
    print("‚ùå Result: Not Significant")

plt.figure(figsize=(10, 6))
sns.kdeplot(df['len_faithful'], label='CDS Faithful', fill=True, color='blue', alpha=0.5)
sns.kdeplot(df['len_distracted'], label='CDS Distracted', fill=True, color='orange', alpha=0.5)
plt.title('Comparison of Length between Faithful and Distracted Distributions')
plt.xlabel('Length')
plt.ylabel('Density')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()

def corr_len_cds_(corr_):
  cc7=0.7; cc5=0.5; cc3=0.3
  if abs(corr_) > cc7:
      print("‚ö†Ô∏è Í∏∏Ïù¥Í∞Ä CDSÏóê Í∞ïÎ†•Ìïú ÏòÅÌñ•ÏùÑ Ï§Ñ Í∞ÄÎä•ÏÑ±Ïù¥ ÏûàÏùå.")
  if abs(corr_) > cc5:
      print("‚ö†Ô∏è Í∏∏Ïù¥Í∞Ä CDSÏóê Ï§ëÍ∞Ñ ÏòÅÌñ•ÏùÑ Ï§Ñ Í∞ÄÎä•ÏÑ±Ïù¥ ÏûàÏùå.")
  elif abs(corr_) > cc3:
      print("‚ö†Ô∏è Í∏∏Ïù¥Í∞Ä CDSÏóê ÏïΩÌïú ÏòÅÌñ•ÏùÑ Ï§Ñ Í∞ÄÎä•ÏÑ±Ïù¥ ÏûàÏùå.")
  else:
      print("‚ÑπÔ∏è Í∏∏Ïù¥ ÏòÅÌñ•ÏùÄ ÌÅ¨ÏßÄ ÏïäÏùÄ Í≤ÉÏúºÎ°ú Î≥¥ÏûÑ.")

corr_len_cds = df['len_diff'].corr(df['cds_diff'])
print(f"Correlation(len_diff, cds_diff): {corr_len_cds:.4f}")
corr_len_cds_(corr_len_cds)
corr_faithful = df['len_faithful'].corr(df['cds_faithful'])
print(f"Correlation(len_f, cds_f): {corr_faithful:.4f}")
corr_len_cds_(corr_faithful)
corr_distracted = df['len_distracted'].corr(df['cds_distracted'])
print(f"Correlation(len_d, cds_d): {corr_distracted:.4f}")
corr_len_cds_(corr_distracted)

# 'diff' Í∞íÏùÑ Í∏∞Ï§ÄÏúºÎ°ú 'diff_group' ÏÉùÏÑ±
def assign_diff_group(diff):
    if diff > 0:
        return 'positive'
    elif diff < 0:
        return 'negative'
    else:
        return 'zero'

df['diff_group'] = df['cds_diff'].apply(assign_diff_group)

def assign_diff_group(diff):
    if diff > 0:
        return 'positive'
    elif diff < 0:
        return 'negative'
    else:
        return 'zero'

df['len_diff_group'] = df['len_diff'].apply(assign_diff_group)

# ÏãúÍ∞ÅÌôî ÏÑ§Ï†ï: Îëê Í∞úÏùò ÌîåÎ°ØÏùÑ ÌïòÎÇòÏùò Í∑∏Î¶ºÏóê Î∞∞Ïπò
def two_plots(df, plot_var):
  fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=False)
  cols1 = {'positive': 'blue', 'negative': 'red', 'zero': 'black'}
  cols2 = {'positive': 'green', 'negative': 'orange', 'zero': 'black'}

  # Ï≤´ Î≤àÏß∏ ÌîåÎ°Ø: cds_faithful vs cds_distracted
  sns.scatterplot(
      ax=axes[0],
      #data=df[df['len_diff']<0], #len_faithful<len_distracted,
      #data=df[df['len_diff']>0], #len_faithful>len_distracted,
      data=df, x=plot_var["x1"], y=plot_var["y1"], hue=plot_var["hue1"],
      #x='cds_faithful',      #y='cds_distracted',      #hue='diff_group',
      palette=cols1,
      legend='full'
  )
  axes[0].set_title(plot_var["title1"])
  axes[0].set_xlabel(plot_var["xlabel1"])
  axes[0].set_ylabel(plot_var["ylabel1"])
  axes[0].grid(True)

  # Îëê Î≤àÏß∏ ÌîåÎ°Ø: len_faithful vs len_distracted
  sns.scatterplot(
      ax=axes[1],
      #data=df[df['len_diff']<0], #len_faithful<len_distracted,
      #data=df[df['len_diff']>0], #len_faithful>len_distracted,
      data=df, x=plot_var["x2"], y=plot_var["y2"], hue=plot_var["hue2"],
      #x='len_faithful',      #y='len_distracted',      #hue='len_diff_group',
      palette=cols1 if plot_var["hue1"]==plot_var["hue2"] else cols2,
      legend='full'
  )
  axes[1].set_title(plot_var["title2"])
  axes[1].set_xlabel(plot_var["xlabel2"])
  axes[1].set_ylabel(plot_var["ylabel2"])
  axes[1].grid(True)

  plt.tight_layout()
  plt.show()

# Ï†êÏàò Ï∞®Ïù¥ÏôÄ Ï†êÏàò, Í∏∏Ïù¥ Ï∞®Ïù¥ÏôÄ Í∏∏Ïù¥: ÎãπÏó∞Ìûà ÎÇòÎâòÏñ¥Ïßê
plot_var1 = {"title1":'Scatter Plot of Faithful vs Distracted Centeredness',
             "xlabel1": 'Faithful Centeredness', "ylabel1": 'Distracted Centeredness',
             "x1": 'cds_faithful', "y1": 'cds_distracted', "hue1": 'diff_group',
             "title2":'Scatter Plot of Faithful vs Distracted Length',
             "xlabel2": 'Faithful Length', "ylabel2": 'Distracted Length',
             "x2": 'len_faithful', "y2": 'len_distracted', "hue2": 'len_diff_group'}
two_plots(df, plot_var1)

#
plot_var2 = {"title1":'Scatter Plot of Faithful vs Distracted Centeredness',
             "xlabel1": 'Faithful Centeredness', "ylabel1": 'Distracted Centeredness',
             "x1": 'cds_faithful', "y1": 'cds_distracted', "hue1": 'len_diff_group',
             "title2":'Scatter Plot of Faithful vs Distracted Length',
             "xlabel2": 'Faithful Length', "ylabel2": 'Distracted Length',
             "x2": 'len_faithful', "y2": 'len_distracted', "hue2": 'diff_group'}
two_plots(df, plot_var2)

#
plot_var3 = {"title1":'Scatter Plot of Faithful vs Distracted Centeredness',
             "xlabel1": 'Distracted Length', "ylabel1": 'Distracted Centeredness',
             "x1": 'len_distracted', "y1": 'cds_distracted', "hue1": 'len_diff_group',
             "title2":'Scatter Plot of Faithful vs Distracted Length',
             "xlabel2": 'Faithful Length', "ylabel2": 'Faithful Centeredness',
             "x2": 'len_faithful', "y2": 'cds_faithful', "hue2": 'len_diff_group'}
two_plots(df, plot_var3)

#
plot_var4 = {"title1":'Scatter Plot of Faithful vs Distracted Centeredness',
             "xlabel1": 'Distracted Length', "ylabel1": 'Distracted Centeredness',
             "x1": 'len_distracted', "y1": 'cds_distracted', "hue1": 'diff_group',
             "title2":'Scatter Plot of Faithful vs Distracted Length',
             "xlabel2": 'Faithful Length', "ylabel2": 'Faithful Centeredness',
             "x2": 'len_faithful', "y2": 'cds_faithful', "hue2": 'diff_group'}
two_plots(df, plot_var4)

#
plot_var5 = {"title1":'Scatter Plot of Length vs Distracted Centeredness',
             "xlabel1": 'Difference of  Length', "ylabel1": 'Distracted Centeredness',
             "x1": 'len_diff', "y1": 'cds_distracted', "hue1": 'diff_group',
             "title2":'Scatter Plot of Length vs Faithful Centeredness',
             "xlabel2": 'Difference of  Length', "ylabel2": 'Faithful Centeredness',
             "x2": 'len_diff', "y2": 'cds_faithful', "hue2": 'diff_group'}
two_plots(df, plot_var5)

#
plot_var6 = {"title1":'Scatter Plot of CDS vs Distracted Length',
             "xlabel1": 'Difference of CDS', "ylabel1": 'Distracted Length',
             "x1": 'cds_diff', "y1": 'len_distracted', "hue1": 'len_diff_group',
             "title2":'Scatter Plot of CDS vs Faithful Length',
             "xlabel2": 'Difference of CDS', "ylabel2": 'Faithful Length',
             "x2": 'cds_diff', "y2": 'len_faithful', "hue2": 'len_diff_group'}
two_plots(df, plot_var6)

# CDS ÎπÑÍµê
t_stat, p_val = stats.ttest_rel(df['cds_faithful'], df['cds_distracted'])
print("\n" + "="*50)
print("üìä [Phase 1] Quantitative Results")
print("="*50)
print(f"Mean CDS (Faithful):     {df['cds_faithful'].mean():.4f} (std: {df['cds_faithful'].std():.4f})")
print(f"Mean CDS (Distracted): {df['cds_distracted'].mean():.4f} (std: {df['cds_distracted'].std():.4f})")
print(f"Gap (Faithful - Distracted): {df['cds_diff'].mean():.4f}")
print("-" * 50)
print(f"Statistical Significance (Paired T-test):")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value:     {p_val:.4e}")
if p_val < 0.05:
    print("‚úÖ Result: Statistically Significant (p < 0.05)")
else:
    print("‚ùå Result: Not Significant")

plt.figure(figsize=(10, 6))
sns.kdeplot(df['cds_faithful'], label='CDS Faithful', fill=True, color='blue', alpha=0.5)
sns.kdeplot(df['cds_distracted'], label='CDS Distracted', fill=True, color='orange', alpha=0.5)
plt.title('Comparison of CDS Faithful and CDS Distracted Distributions')
plt.xlabel('Score')
plt.ylabel('Density')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()
png_name = file_name.split('/')[-1].split(".")[0]+".png"
plt.savefig(png_name, dpi=300)
print("‚úÖ Figure 1 Saved.")

"""##### ÏãúÍ∞ÅÌôî(4-2)

"""

import json
file_name = file_list_json[11]; print(file_name)
with open(file_name, 'r') as f:
    best_result = json.load(f)

import numpy as np
# Convert the loaded lists back to numpy arrays if they are intended to be used as such later
best_result['attn_f'] = np.array(best_result['attn_f'])
best_result['attn_d'] = np.array(best_result['attn_d'])

tok_f, score_f = best_result['tok_f'], best_result['score_f']
tok_d, score_d = best_result['tok_d'], best_result['score_d']

#type(best_result['attn_f']), len(best_result['attn_f']), best_result['attn_f'][0]

import re

# ÌäπÏàòÎ¨∏Ïûê ÌÜ†ÌÅ∞ ÌïÑÌÑ∞ÎßÅ Ìï®Ïàò
def is_valid_token(token):
    """
    Ïú†Ìö®Ìïú ÌÜ†ÌÅ∞Ïù∏ÏßÄ ÌôïÏù∏ (ÏïåÌååÎ≤≥, Ïà´Ïûê, ÌïúÍ∏Ä Ìè¨Ìï®)
    """
    # ÏïåÌååÎ≤≥, Ïà´Ïûê, ÌïúÍ∏ÄÎßå Ìè¨Ìï®Îêú ÌÜ†ÌÅ∞Îßå ÌóàÏö©
    return bool(re.match(r'^[a-zA-Z0-9Í∞Ä-Ìû£]+$', token.strip()))

# f Îç∞Ïù¥ÌÑ∞ ÌïÑÌÑ∞ÎßÅ Î∞è Ï†ïÎ†¨
filtered_f_data = [(tok, score) for tok, score in zip(tok_f, score_f) if is_valid_token(tok)]
sorted_f_data = sorted(filtered_f_data, key=lambda x: x[1], reverse=True)
sorted_tok_f = [item[0] for item in sorted_f_data]
sorted_score_f = [item[1] for item in sorted_f_data]

# d Îç∞Ïù¥ÌÑ∞ ÌïÑÌÑ∞ÎßÅ Î∞è Ï†ïÎ†¨
filtered_d_data = [(tok, score) for tok, score in zip(tok_d, score_d) if is_valid_token(tok)]
sorted_d_data = sorted(filtered_d_data, key=lambda x: x[1], reverse=True)
sorted_tok_d = [item[0] for item in sorted_d_data]
sorted_score_d = [item[1] for item in sorted_d_data]

import matplotlib.pyplot as plt
import seaborn as sns

# ----------------------------------------------------------
# 6) Heatmap ÏãúÍ∞ÅÌôî
# ----------------------------------------------------------
def show_heatmap(tokens, scores, title="Attribution Heatmap"):

    plt.figure(figsize=(18, 3))
    colors = plt.cm.Reds([s / max(scores) for s in scores])

    for i, (tok, sc) in enumerate(zip(tokens, scores)):
        plt.text(i, 0, tok, rotation=90,
                 fontsize=16,
                 bbox=dict(facecolor=colors[i], alpha=0.8, edgecolor='none'))

    plt.title(title)
    plt.xticks([])
    plt.yticks([])
    plt.show()

# ----------------------------------------------------------
# 7) Score Line Plot
# ----------------------------------------------------------
def show_score_plot(scores, title="Attribution Score Plot"):
    plt.figure(figsize=(18, 3))
    plt.plot(scores)
    plt.title(title)
    plt.xlabel("Token Index")
    plt.ylabel("Importance")
    plt.grid(alpha=0.2)
    plt.show()


# ÏãúÍ∞ÅÌôî
k = 30
print(sorted_tok_f[:k]+sorted_tok_f[-k:])#, sorted_score_f[:k])
show_heatmap(sorted_tok_f[:k]+sorted_tok_f[-k:], sorted_score_f[:k]+sorted_score_f[-k:], title="Faithful Context Heatmap")
show_score_plot(score_f, title="Faithful Context Score Plot")

print(sorted_tok_d[:k]+sorted_tok_f[-k:])#, sorted_score_d[:k])
show_heatmap(sorted_tok_d[:k], sorted_score_d[:k], title="Distracted Context Heatmap")
show_score_plot(score_d, title="Distracted Context Score Plot")

def find_context_question_spans(tokens):
    context_key = "Context"
    question_key = "Question"

    ctx_idx = next((i for i, t in enumerate(tokens) if context_key in t), None)
    q_idx = next((i for i, t in enumerate(tokens) if question_key in t), None)

    # Î™ª Ï∞æÏúºÎ©¥ Ï†àÎ∞ò/Ï†àÎ∞òÏúºÎ°ú split
    if ctx_idx is None or q_idx is None:
        mid = len(tokens) // 2
        return (1, mid), (mid, len(tokens))

    return (ctx_idx + 1, q_idx), (q_idx + 1, len(tokens))


def token_importance_heatmap(tokens, scores, ctx_span, q_span, title="Token Importance Heatmap"):

    # 1 x N ÌòïÌÉú array
    data = np.array(scores).reshape(1, -1)

    # Í∏∞Î≥∏ grayscale heatmap
    plt.figure(figsize=(max(len(tokens) / 10, 20), 2))
    ax = sns.heatmap(
        data,
        cmap="Greys",
        xticklabels=False,
        yticklabels=False,
        cbar=True
    )

    # Context / Question span
    c_s, c_e = ctx_span
    q_s, q_e = q_span

    # Context ÏòÅÏó≠ (ÌååÎûÄÏÉâ Ìà¨Î™Ö)
    ax.add_patch(plt.Rectangle(
        (c_s, 0),             # (x, y)
        c_e - c_s,            # width
        1,                    # height
        fill=True,
        color=(0.3, 0.5, 1, 0.25),
        lw=0
    ))

    # Question ÏòÅÏó≠ (Îπ®Í∞ÑÏÉâ Ìà¨Î™Ö)
    ax.add_patch(plt.Rectangle(
        (q_s, 0),
        q_e - q_s,
        1,
        fill=True,
        color=(1, 0.3, 0.3, 0.25),
        lw=0
    ))

    plt.title(title)
    plt.tight_layout()
    plt.show()


ctx_span, q_span = find_context_question_spans(tok_f)

token_importance_heatmap(tok_f, score_f, ctx_span, q_span,
                         title="Token Importance Heatmap Example")

ctx_span, q_span = find_context_question_spans(tok_d)

token_importance_heatmap(tok_d, score_d, ctx_span, q_span,
                         title="Token Importance Heatmap Example")

def show_heatmap_sns(tokens, scores, ctx_span, q_span, title="Token Attribution Heatmap"):

    # 1 x N Î∞∞Ïó¥ ÏÉùÏÑ±
    scores_arr = np.array(scores).reshape(1, -1)

    # Í∏∞Î≥∏ heatmap ÏÉâÏÉÅÏùÄ grayscaleÎ°ú
    base_colors = sns.light_palette("gray", as_cmap=True)

    # heatmap Ï∂úÎ†•
    plt.figure(figsize=(max(len(tokens)/10, 20), 2))
    ax = sns.heatmap(
        scores_arr,
        cmap=base_colors,
        cbar=True,
        xticklabels=False,
        yticklabels=False
    )

    # Context / Question Íµ¨Í∞Ñ ÏÉâ Ïò§Î≤ÑÎ†àÏù¥
    c_s, c_e = ctx_span
    q_s, q_e = q_span

    # Context ÏòÅÏó≠ (ÌååÎûÄÏÉâ Î∞òÌà¨Î™Ö)
    ax.add_patch(plt.Rectangle(
        (c_s, 0),            # (x, y)
        c_e - c_s,           # width
        1,                   # height
        fill=True,
        color=(0.3, 0.5, 1, 0.25),
        lw=0
    ))

    # Question ÏòÅÏó≠ (Îπ®Í∞ÑÏÉâ Î∞òÌà¨Î™Ö)
    ax.add_patch(plt.Rectangle(
        (q_s, 0),
        q_e - q_s,
        1,
        fill=True,
        color=(1, 0.3, 0.3, 0.25),
        lw=0
    ))

    plt.title(title)
    plt.tight_layout()
    plt.show()

def show_heatmap_ctx_q(tokens, scores, ctx_span, q_span, title="Attribution Heatmap"):

    c_s, c_e = ctx_span
    q_s, q_e = q_span
    max_score = max(scores)

    plt.figure(figsize=(22, 3))

    for i, (tok, sc) in enumerate(zip(tokens, scores)):
        # Ï†ïÍ∑úÌôî
        norm_sc = sc / max_score if max_score > 0 else 0

        # Î¨∏Îß•/ÏßàÎ¨∏ Íµ¨Î∂Ñ ÏÉâÏÉÅ Ï†ÅÏö©
        if c_s <= i < c_e:
            color = plt.cm.Blues(norm_sc)   # Î¨∏Îß• (ÌååÎûÄ Í≥ÑÏó¥)
        elif q_s <= i < q_e:
            color = plt.cm.Reds(norm_sc)    # ÏßàÎ¨∏ (Îπ®Í∞Ñ Í≥ÑÏó¥)
        else:
            color = plt.cm.Greys(0.3)       # ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ ÏòÅÏó≠ Îì±ÏùÄ ÌöåÏÉâ

        plt.text(
            i, 0, tok, rotation=90, fontsize=6,
            bbox=dict(facecolor=color, alpha=0.85, edgecolor='none')
        )

    plt.title(title)
    plt.xticks([])
    plt.yticks([])
    plt.show()

def show_score_plot_ctx_q(scores, ctx_span, q_span, title="Attribution Score Plot"):
    c_s, c_e = ctx_span
    q_s, q_e = q_span

    plt.figure(figsize=(22, 4))

    # Î¨∏Îß• ÏòÅÏó≠ Î∞∞Í≤ΩÏÉâ
    plt.axvspan(c_s, c_e, color='skyblue', alpha=0.25, label="Context")

    # ÏßàÎ¨∏ ÏòÅÏó≠ Î∞∞Í≤ΩÏÉâ
    plt.axvspan(q_s, q_e, color='lightcoral', alpha=0.25, label="Question")

    plt.plot(scores, color='black', linewidth=1)
    plt.scatter(range(len(scores)), scores, color='green', s=10)

    plt.title(title)
    plt.xlabel("Token Index")
    plt.ylabel("Importance Score")
    plt.legend()
    plt.grid(alpha=0.2)
    plt.show()


ctx_f, q_f = find_context_question_spans(tok_f)
ctx_d, q_d = find_context_question_spans(tok_d)

# Faithful heatmap
show_heatmap_ctx_q(tok_f, score_f, ctx_f, q_f,
                   title="Faithful Attribution Heatmap")
show_heatmap_sns(tok_f, score_f, ctx_f, q_f,
                 title="Faithful Heatmap (Seaborn)")

# Faithful score plot
show_score_plot_ctx_q(score_f, ctx_f, q_f,
                      title="Faithful Attribution Score Plot")

# Distracted heatmap
show_heatmap_ctx_q(tok_d, score_d, ctx_d, q_d,
                   title="Distracted Attribution Heatmap")
show_heatmap_sns(tok_d, score_d, ctx_d, q_d,
                   title="Distracted Attribution Heatmap")

# Distracted score plot
show_score_plot_ctx_q(score_d, ctx_d, q_d,
                      title="Distracted Attribution Score Plot")

"""##### backup"""

# ÏãúÍ∞ÅÌôî ÏÑ§Ï†ï
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=df,#[df['len_diff']<0], #len_faithful<len_distracted
    x='len_diff',
    y='cds_faithful',
    hue='diff_group', #cds_faithful-cds_distracted
    palette={'positive': 'blue', 'negative': 'red'},
    legend='full'
)

# Í∑∏ÎûòÌîÑ Ï†úÎ™© Î∞è Ï∂ï Î†àÏù¥Î∏î ÏÑ§Ï†ï
plt.title('Scatter Plot of Faithful vs Distracted Centeredness')
plt.xlabel('Faithful Centeredness')
plt.ylabel('Distracted Centeredness')

# Î≤îÎ°Ä ÌëúÏãú
plt.legend(title='Diff Group')
plt.grid(True)
plt.show()

# ÏãúÍ∞ÅÌôî ÏÑ§Ï†ï: Îëê Í∞úÏùò ÌîåÎ°ØÏùÑ ÌïòÎÇòÏùò Í∑∏Î¶ºÏóê Î∞∞Ïπò
fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=False)

# Ï≤´ Î≤àÏß∏ ÌîåÎ°Ø: cds_faithful vs cds_distracted
sns.scatterplot(
    ax=axes[0],
    #data=df[df['len_diff']<0], #len_faithful<len_distracted,
    data=df[df['len_diff']>0], #len_faithful>len_distracted,
    x='cds_faithful',
    y='cds_distracted',
    hue='diff_group',
    palette={'positive': 'blue', 'negative': 'red'},
    legend='full'
)
axes[0].set_title('Scatter Plot of Faithful vs Distracted Centeredness')
axes[0].set_xlabel('Faithful Centeredness')
axes[0].set_ylabel('Distracted Centeredness')
axes[0].grid(True)

# Îëê Î≤àÏß∏ ÌîåÎ°Ø: len_faithful vs len_distracted
sns.scatterplot(
    ax=axes[1],
    #data=df[df['len_diff']<0], #len_faithful<len_distracted,
    data=df[df['len_diff']>0], #len_faithful>len_distracted,
    x='len_faithful',
    y='len_distracted',
    hue='len_diff_group',
    palette={'positive': 'green', 'negative': 'orange'},
    legend='full'
)
axes[1].set_title('Scatter Plot of Faithful vs Distracted Length')
axes[1].set_xlabel('Faithful Length')
axes[1].set_ylabel('Distracted Length')
axes[1].grid(True)

plt.tight_layout()
plt.show()

# ÏãúÍ∞ÅÌôî ÏÑ§Ï†ï: Îëê Í∞úÏùò ÌîåÎ°ØÏùÑ ÌïòÎÇòÏùò Í∑∏Î¶ºÏóê Î∞∞Ïπò
fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=False)

# Ï≤´ Î≤àÏß∏ ÌîåÎ°Ø: cds_faithful vs cds_distracted
sns.scatterplot(
    ax=axes[0],
    #data=df[df['len_diff']<0], #len_faithful<len_distracted,
    data=df[df['len_diff']>0], #len_faithful>len_distracted,
    y='cds_faithful',
    x='len_faithful',
    hue='diff_group',
    palette={'positive': 'blue', 'negative': 'red'},
    legend='full'
)
axes[0].set_title('Scatter Plot of Faithful vs Distracted Centeredness')
axes[0].set_ylabel('Faithful Centeredness')
axes[0].set_xlabel('Faithful Length')
axes[0].grid(True)

# Îëê Î≤àÏß∏ ÌîåÎ°Ø: len_faithful vs len_distracted
sns.scatterplot(
    ax=axes[1],
    #data=df[df['len_diff']<0], #len_faithful<len_distracted,
    data=df[df['len_diff']>0], #len_faithful>len_distracted,
    y='cds_faithful',
    x='len_faithful',
    hue='len_diff_group',
    palette={'positive': 'green', 'negative': 'orange'},
    legend='full'
)
axes[1].set_title('Scatter Plot of Faithful vs Distracted Length')
axes[1].set_ylabel('Faithful Centeredness')
axes[1].set_xlabel('Faithful Length')
axes[1].grid(True)

plt.tight_layout()
plt.show()

# ÏãúÍ∞ÅÌôî ÏÑ§Ï†ï: Îëê Í∞úÏùò ÌîåÎ°ØÏùÑ ÌïòÎÇòÏùò Í∑∏Î¶ºÏóê Î∞∞Ïπò
fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=False)

# Ï≤´ Î≤àÏß∏ ÌîåÎ°Ø: cds_faithful vs cds_distracted
sns.scatterplot(
    ax=axes[0],
    #data=df[df['len_diff']<0], #len_faithful<len_distracted,
    data=df[df['len_diff']>0], #len_faithful>len_distracted,
    y='cds_distracted',
    x='len_distracted',
    hue='diff_group',
    palette={'positive': 'blue', 'negative': 'red'},
    legend='full'
)
axes[0].set_title('Scatter Plot of Faithful vs Distracted Centeredness')
axes[0].set_ylabel('Distracted Centeredness')
axes[0].set_xlabel('Distracted Length')
axes[0].grid(True)

# Îëê Î≤àÏß∏ ÌîåÎ°Ø: len_faithful vs len_distracted
sns.scatterplot(
    ax=axes[1],
    #data=df[df['len_diff']<0], #len_faithful<len_distracted,
    data=df[df['len_diff']>0], #len_faithful>len_distracted,
    y='cds_distracted',
    x='len_distracted',
    hue='len_diff_group',
    palette={'positive': 'green', 'negative': 'orange'},
    legend='full'
)
axes[1].set_title('Scatter Plot of Faithful vs Distracted Length')
axes[1].set_ylabel('Distracted Centeredness')
axes[1].set_xlabel('Distracted Length')
axes[1].grid(True)

plt.tight_layout()
plt.show()

# ==========================================
# 4. Ï†ïÏÑ± Î∂ÑÏÑù ÏãúÍ∞ÅÌôî (Figure 1 & 2)
# ==========================================
if best_viz_candidate:
    sns.set_theme(style="white")
    plt.rcParams['font.family'] = 'serif'
    # --- Figure 1: Heatmap Comparison (Intensity) ---
    fig1, axes = plt.subplots(2, 1, figsize=(12, 5))

    def draw_heatmap(ax, tokens, scores, title):
      limit = 60
      clean = [t.replace(' ', '').replace('ƒ†', '').replace('<', '').replace('>', '').replace('|', '').replace('user', '').replace('Context', '') for t in tokens if t != ''][7:limit]
      vals = np.array(scores)[7:limit].reshape(1, -1)
      vals = (vals - vals.min()) / (vals.max() - vals.min() + 1e-9) # Normalize

      sns.heatmap(vals, xticklabels=clean, yticklabels=False, cmap="Reds", ax=ax, cbar=False)
      ax.set_title(title, loc='left', fontsize=12, fontweight='bold')
      ax.set_xticklabels(clean, rotation=45, ha='right', fontsize=8)

    draw_heatmap(axes[0], best_viz_candidate['tok_f'], best_viz_candidate['score_f'], "(a) Faithful: High Attribution on Context")
    draw_heatmap(axes[1], best_viz_candidate['tok_h'], best_viz_candidate['score_h'], "(b) Distracted: Attribution Shifted to Question")

    plt.tight_layout()
    plt.savefig("fig1_heatmap.png", dpi=300)
    print("‚úÖ Figure 1 Saved.")

    # --- Figure 2: Attention Map Comparison (Structure) ---
    # Ïñ¥ÌÖêÏÖò ÎßµÏùÄ N x N Ïù¥ÎØÄÎ°ú ÏûòÎùºÏÑú(Crop) Î≥¥Ïó¨Ï£ºÎäî Í≤ÉÏù¥ Ï¢ãÏùå
    fig2, axes = plt.subplots(1, 2, figsize=(14, 6))

    def draw_attn_map(ax, tokens, attn_mat, title):
      limit = 40 # 40x40 ÌÜ†ÌÅ∞Îßå ÏãúÍ∞ÅÌôî (Í∞ÄÎèÖÏÑ± ÏúÑÌï®)
      clean = [t.replace(' ', '').replace('ƒ†', '').replace('<', '').replace('>', '').replace('|', '').replace('user', '').replace('Context', '') for t in tokens if t != ''][7:limit]
      mat = attn_mat[7:limit, 7:limit]

      sns.heatmap(mat, xticklabels=clean, yticklabels=clean, cmap="Blues", ax=ax, cbar=False, square=True)
      ax.set_title(title, fontsize=12, fontweight='bold')
      ax.tick_params(axis='x', rotation=90, labelsize=7)
      ax.tick_params(axis='y', rotation=0, labelsize=7)

    draw_attn_map(axes[0], best_viz_candidate['tok_f'], best_viz_candidate['attn_f'], "(a) Faithful Attention Structure")
    draw_attn_map(axes[1], best_viz_candidate['tok_h'], best_viz_candidate['attn_h'], "(b) Distracted Attention Structure")

    plt.tight_layout()
    plt.savefig("fig2_attention_map.png", dpi=300)
    print("‚úÖ Figure 2 Saved.")

best_result['f']

best_result['q']

best_result['d']

# ==========================================
# 4. Ï†ïÏÑ± Î∂ÑÏÑù: Figure 1 ÌûàÌä∏Îßµ ÏÉùÏÑ±
# ==========================================
if best_viz_candidate:
    print(f"\nüé® Generating Figure 1 from Sample ID (Best Gap)...")

    sns.set_theme(style="white")
    plt.rcParams['font.family'] = 'serif'
    fig, axes = plt.subplots(2, 1, figsize=(12, 5), sharex=False)
    def draw_row(ax, tokens, scores, title):
        # ÏãúÍ∞ÅÌôî Í∏∏Ïù¥ Ï†úÌïú
        limit = 70
        clean_toks = [t.replace(' ', '').replace('ƒ†', '') for t in tokens][7:limit]
        norm_scores = np.array(scores)[7:limit]
        # Ï†ïÍ∑úÌôî
        if norm_scores.max() > 0:
            norm_scores /= norm_scores.max()
        sns.heatmap(
            norm_scores.reshape(1, -1),
            xticklabels=clean_toks, yticklabels=False,
            cmap="Reds", ax=ax, cbar=True,
            cbar_kws={"orientation": "vertical", "shrink": 0.8}
        )
        ax.set_title(title, fontsize=12, fontweight='bold', loc='left')
        ax.set_xticklabels(clean_toks, rotation=45, ha='right', fontsize=8)
    draw_row(axes[0], best_viz_candidate['tok_f'], best_viz_candidate['score_f'],
             f"(a) Faithful Generation (Correct Context) - High Attention on Context")
    draw_row(axes[1], best_viz_candidate['tok_h'], best_viz_candidate['score_h'],
             f"(b) Hallucination Risk (Irrelevant Context) - Attention Shifted to Question")

    plt.tight_layout()
    plt.savefig("phase1_figure1_heatmap.png", bbox_inches='tight', dpi=300)
    print("‚úÖ Saved 'phase1_figure1_heatmap.png'")

print("\nüéâ Phase 1 Experiment Complete.")

import numpy as np
import matplotlib.pyplot as plt
from IPython.display import HTML, display

import re

def clean_token(tok):
    """
    Î∂àÌïÑÏöîÌïú BPE ÌÜ†ÌÅ∞ Ï†ëÎëêÏñ¥(ƒ†, ƒä, ‚ñÅ Îì±)ÏôÄ ÌäπÏàòÎ¨∏ÏûêÎ•º Ï†úÍ±∞/Ï†ïÎ¶¨ÌïòÎäî Ìï®Ïàò.
    """

    # 1. BPE prefix Ï†úÍ±∞
    tok = tok.replace("ƒ†", " ")      # GPT-2 Í≥ÑÏó¥ Í≥µÎ∞± ÌÜ†ÌÅ∞
    tok = tok.replace("ƒä", " ")      # Line-break Í≥ÑÏó¥
    tok = tok.replace("‚ñÅ", " ")      # SentencePiece Í≥µÎ∞±

    # 2. HTML-safe Ï≤òÎ¶¨ (ÏõêÌïòÎäî Í≤ΩÏö∞)
    tok = tok.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

    # 3. ÏñëÏ™Ω Í≥µÎ∞± Ï†úÍ±∞
    tok = tok.strip()

    # 4. ÌäπÏàòÎ¨∏Ïûê ÌÅ¥Î¶∞ÏóÖ (Î¨∏Ïû•Î∂ÄÌò∏Îäî Ïú†ÏßÄ)
    tok = re.sub(r"[^A-Za-z0-9Í∞Ä-Ìû£ ,.?!%:/\-_=+()]", "", tok)

    # 5. Îã§Ïãú Ìïú Î≤à trim
    tok = tok.strip()

    return tok if tok != "" else " "

def split_tokens_to_sentences(tokens, scores):
    sentences = []
    curr_toks = []
    curr_scores = []

    end_tokens = [".", "?", "!"]

    for t, s in zip(tokens, scores):
        curr_toks.append(t)
        curr_scores.append(s)

        if t in end_tokens or t.endswith(tuple(end_tokens)):
            sentences.append((curr_toks, curr_scores))
            curr_toks, curr_scores = [], []

    if curr_toks:
        sentences.append((curr_toks, curr_scores))

    return sentences


def html_color_sentence(tokens, scores, color):
    """
    ÌïòÎÇòÏùò Î¨∏Ïû•ÏùÑ HTML Ïª¨Îü¨ spanÏúºÎ°ú Î≥ÄÌôò
    """
    html = ""
    for t, s in zip(tokens, scores):
        t_clean = clean_token(t)
        html += f"<span style='background-color:{color}; padding:2px; margin:1px; border-radius:4px;'>{t_clean}</span> "
    return html


def visualize_sentence_html_interactive(tokens, scores, sep_idx, top_k=3):
    scores = np.array(scores)

    #---------------------------
    # 1. Context / Question Î∂ÑÎ¶¨
    #---------------------------
    context_tokens = tokens[:sep_idx]
    context_scores = scores[:sep_idx]

    question_tokens = tokens[sep_idx:]
    question_scores = scores[sep_idx:]

    #---------------------------
    # 2. Context Î¨∏Ïû• Îã®ÏúÑ
    #---------------------------
    sentences = split_tokens_to_sentences(context_tokens, context_scores)
    sentence_scores = np.array([sum(s[1]) for s in sentences])

    # Top-K Î¨∏Ïû• ÏÑ†ÌÉù
    if len(sentence_scores) > 0:
        top_indices = np.argsort(sentence_scores)[-top_k:]
    else:
        top_indices = []

    # Normalize for colormap
    if len(sentence_scores) > 0:
        norm = (sentence_scores - sentence_scores.min()) / (sentence_scores.max() - sentence_scores.min() + 1e-8)
    else:
        norm = []

    cmap = plt.cm.get_cmap("coolwarm")

    #---------------------------
    # 3. HTML ÏÉùÏÑ±
    #---------------------------
    html_output = "<h3>Sentence-level IG Highlight (Context Top-{}</h3>".format(top_k)
    html_output += "<div style='font-size: 16px; line-height: 2;'>"

    # Context Ï∂úÎ†•
    i=0
    html_output += "<h4>Context (Top Sentences Highlighted)</h4>"
    for idx, (sent_tokens, sent_scores) in enumerate(sentences):
        if idx in top_indices and i<3:
            i = i+1
            # Ï§ëÏöî Î¨∏Ïû• ‚Üí Ïª¨Îü¨Îßµ Í∏∞Î∞ò Í∞ïÏ°∞
            rgba = cmap(norm[idx])
            color = f"rgba({int(rgba[0]*255)}, {int(rgba[1]*255)}, {int(rgba[2]*255)}, 0.6)"
        else:
            # ÎπÑÏ§ëÏöî Î¨∏Ïû• ‚Üí ÌöåÏÉâ
            color = "rgba(200,200,200,0.3)"

        if idx in top_indices:
            html_output += html_color_sentence(sent_tokens, sent_scores, color)
            html_output += "<br><br>"

    # Question Ï∂úÎ†•
    html_output += "<h4>Question</h4>"
    for t, s in zip(question_tokens, question_scores):
        t_clean = clean_token(t)
        html_output += f"<span style='background-color:rgba(255,150,150,0.7); padding:2px; margin:1px; border-radius:4px;'>{t_clean}</span> "
    html_output += "</div>"

    display(HTML(html_output))

    #---------------------------
    # 4. Í∑∏ÎûòÌîÑ: Top-K Î¨∏Ïû•Îßå ÌëúÏãú
    #---------------------------
    plt.figure(figsize=(18, 5))

    x_pos = 0
    xticks = []
    xlabels = []

    for idx in top_indices:
        sent_tokens, sent_scores = sentences[idx]
        clean_tokens = []
        for t in sent_tokens:
            clean_tokens.append(clean_token(t))
        xs = np.arange(x_pos, x_pos + len(clean_tokens))

        rgba = cmap(norm[idx])
        color = (rgba[0], rgba[1], rgba[2], 0.8)

        plt.bar(xs, sent_scores, color=color)

        xticks.extend(xs)
        xlabels.extend(clean_tokens)
        x_pos += len(sent_tokens)

    plt.title(f"Top {top_k} Context Sentences (Token-Level IG Scores)")
    plt.xticks(xticks, xlabels, rotation=90)
    plt.xlabel("Tokens")
    plt.ylabel("Attribution Score")
    plt.tight_layout()
    plt.show()



# ÏòàÏãú Ïã§Ìñâ
# tokens, scores, cds, raw_attn = get_phi3_token_importance(context, question, model, tokenizer)
sep_idx = next(i for i, t in enumerate(best_viz_candidate['tok_f']) if "Question" in t)
visualize_sentence_html_interactive(best_viz_candidate['tok_f'], best_viz_candidate['score_f'], sep_idx, top_k=5)

import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
with open('/content/drive/MyDrive/ColabNotebooks/datasets/phase1_experiment_results.csv', 'r') as f:
    df = pd.read_csv(f)

plt.figure(figsize=(10, 6))
sns.kdeplot(df['cds_faithful'], label='CDS Faithful', fill=True, color='blue', alpha=0.5)
sns.kdeplot(df['cds_distracted'], label='CDS Distracted', fill=True, color='orange', alpha=0.5)
plt.title('Comparison of CDS Faithful and CDS Distracted Distributions')
plt.xlabel('Score')
plt.ylabel('Density')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()
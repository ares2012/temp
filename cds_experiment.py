# -*- coding: utf-8 -*-
"""cds_experiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/ares2012/temp/blob/master/cds_experiment.ipynb

# ENV

<a target="_blank" href="https://colab.research.google.com/github/ares2012/temp/blob/master/Open_in_Colab.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

https://openincolab.com
"""

from google.colab import drive
drive.mount('/content/drive')

"""##### API_KEY"""

import os
import json

import numpy as np
import pandas as pd

cp -r /content/drive/MyDrive/Colab\ Notebooks/my_list.txt /content

file_path = '/content/my_list.txt'

with open(file_path, 'r') as f:
  keys = json.load(f)

oKEY = keys['OPENAI_API_KEY']
pKEY = keys['PINECONE_API_KEY']
hKEY = keys['huggingface.co']

!pip install openai --upgrade --quiet

"""##### Successfully installed transformers-4.57.3


* pip install captum bitsandbytes llmlingua
* pip install --upgrade transformers


"""

pip install captum bitsandbytes llmlingua

pip install pycuda

pip install --upgrade transformers

pip install transformers==4.57.3

"""# CDS

"""

import os
import torch

import numpy as np
import pandas as pd
from scipy import stats

import matplotlib.pyplot as plt
import seaborn as sns

import random
from tqdm import tqdm

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import AutoConfig, BitsAndBytesConfig
from captum.attr import LayerIntegratedGradients, IntegratedGradients

os.environ["PYTORCH_CUDA_ALLOC_CON"] = "expandable_segments:True"
#os.environ["PYTORCH_NO_CUDA_MEMORY_CACHING"] = "1"
#os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

! cp /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/072cb7562cb8c4adf682a8e186aaafa49469eb5d/configuration_phi3.py /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/f39ac1d28e925b323eae81227eaba4464caced4e

! cp /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py /content/drive/MyDrive/ColabNotebooks/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/f39ac1d28e925b323eae81227eaba4464caced4e

# ==========================================
# 0. ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è Ïó£ÏßÄ Î™®Îç∏ Î°úÎìú Î°úÏª¨ (1Î∂Ñ)
# ==========================================
torch.cuda.empty_cache()
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

#model_name = "microsoft/Phi-3-mini-128k-instruct" #(1Î∂Ñ, 52Ï¥à)
model_name = "microsoft/Phi-3-mini-4k-instruct" #(1Î∂Ñ, 52Ï¥à)
model_name = "Qwen/Qwen3-0.6B" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)
#model_name = "Qwen/Qwen3-0.6B-Base" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)
#model_name = "Qwen/Qwen3-4B-Instruct-2507" #(25Ï¥à/1.3G, 6Ï¥à/3.5G)
local_path = '/content/drive/MyDrive/ColabNotebooks/hub/'

# Construct the path to the model's base directory in the hub
model_base_dir = os.path.join(local_path, "models--" + model_name.replace("/", "--"))

# Construct the path to the snapshots directory
snapshots_dir = os.path.join(model_base_dir, "snapshots")

# Dynamically get the snapshot hash (assuming there's only one subdirectory in snapshots)
if os.path.exists(snapshots_dir):
    snapshot_hashes = os.listdir(snapshots_dir)
    if snapshot_hashes:
        snapshot_hash = snapshot_hashes[0] # Take the first one found
        # Construct the full local path to the model directory including the dynamically found snapshot hash
        local_model_dir = os.path.join(snapshots_dir, snapshot_hash)
    else:
        raise FileNotFoundError(f"No snapshot hashes found in {snapshots_dir}")
else:
    raise FileNotFoundError(f"Snapshots directory not found: {snapshots_dir}")
print(f"Loading model from: {local_model_dir}")

# Explicitly load the configuration first
config = AutoConfig.from_pretrained(local_model_dir, trust_remote_code=True, local_files_only=True, output_attentions=True)
print(f"Type of config after loading: {type(config)}") # Debugging line

tokenizer = AutoTokenizer.from_pretrained(# model_name,
    local_model_dir, # Use the direct local path
    config=config, # Pass the loaded config
    #trust_remote_code=True
)
print(f"tokenizer.vocab_files_names: {tokenizer.vocab_files_names}")

bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
)
model = AutoModelForCausalLM.from_pretrained(# model_name,
        local_model_dir, # Use the direct local path
        config=config, # Pass the loaded config
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        attn_implementation="eager",
        #output_attentions=True,
        local_files_only = True # Ensure local files are used
)
# This globally disables the problematic caching mechanism.
model.config.use_cache = False

torch.cuda.mem_get_info(), torch.cuda.mem_get_info()[0]

torch.cuda.memory_reserved(0), torch.cuda.memory_allocated(0), torch.cuda.get_device_properties(0).total_memory

import gc
gc.collect(); torch.cuda.empty_cache()
torch.cuda.memory_reserved(0), torch.cuda.memory_allocated(0), torch.cuda.get_device_properties(0).total_memory

"""##### ÌîÑÎ°¨ÌîÑÌä∏


* apply chat model
* context+question


"""

# ==========================================
# 1. XAI: IG Î∂ÑÏÑù Î∞è CDS Í≥ÑÏÇ∞
# ==========================================
from captum.attr import IntegratedGradients
#from torch.cuda import AcceleratorError
#import pycuda.driver as cuda
def get_phi3_token_importance(context, question, model, tokenizer):
    # Î™ÖÏãúÏ†ÅÏù∏ Íµ¨Ï°∞Î•º Í∞ÄÏßÑ ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± (Separator Í∞êÏßÄ Ïö©Ïù¥)
    # Phi-3 ÌÖúÌîåÎ¶ø + Î™ÖÌôïÌïú Íµ¨Î∂ÑÏûê
    full_text = f"<|user|>\nContext:\n{context}\n\nQuestion:\n{question}\n\nAnswer based on the context:<|end|>\n<|assistant|>"

    # 0. ÏûÖÎ†• ID Ï∂îÏ∂ú
    torch.cuda.empty_cache()
    inputs = tokenizer(full_text, return_tensors="pt", add_special_tokens=False).to(DEVICE)
    input_ids = inputs.input_ids
    torch.cuda.empty_cache()

    # 1. Raw Attention Ï∂îÏ∂ú (Forward Pass)
    with torch.no_grad():
      #output = model(**inputs)#; print(type(output))
      output = model(input_ids, output_attentions=True) #  AttentionÎßå Ï∂îÏ∂ú
      # ÎßàÏßÄÎßâ Î†àÏù¥Ïñ¥Ïùò Ïñ¥ÌÖêÏÖò (Batch, Head, Seq, Seq) -> Head ÌèâÍ∑† -> (Seq, Seq)
      raw_attn = output.attentions[-1][0].mean(dim=0).cpu().numpy()
      #raw_attn = output.attentions[0][0].mean(dim=0).cpu().numpy()

    # 2. ÏûÑÎ≤†Îî© Î†àÏù¥Ïñ¥ Ï∞æÍ∏∞
    if hasattr(model, "model") and hasattr(model.model, "embed_tokens"):
        embedding_layer = model.model.embed_tokens
    elif hasattr(model, "embed_tokens"): # Llama/Gemma ÏùºÎ∂Ä
        embedding_layer = model.embed_tokens
    else:
        embedding_layer = model.get_input_embeddings()

    torch.cuda.empty_cache()
    # 3. ÏûÑÎ≤†Îî© Î≤°ÌÑ∞Î•º ÎØ∏Î¶¨ Í≥ÑÏÇ∞ > CaptumÏùò Input
    # (4-bit Î™®Îç∏ Îì±ÏóêÏÑú Gradient Í≥ÑÏÇ∞ÏùÑ ÏúÑÌï¥ requires_grad ÏÑ§Ï†ïÏù¥ ÌïÑÏöîÌï† Ïàò ÏûàÏùå)
    input_embeddings = embedding_layer(input_ids)

    # 4. Forward Ìï®Ïàò Ï†ïÏùò (ÏûÑÎ≤†Îî©ÏùÑ ÏûÖÎ†•ÏúºÎ°ú Î∞õÏùå)
    def forward_func(inputs_embeds):
        # inputs_embedsÎ•º Î™®Îç∏Ïóê ÏßÅÏ†ë Ï£ºÏûÖ
        #outputs = model(inputs_embeds=inputs_embeds)
        outputs = model(inputs_embeds=inputs_embeds, output_attentions=False) # attention ÎπÑÌôúÏÑ±Ìôî
        #return outputs.logits[0, -1, :].max().unsqueeze(0)
        return outputs.logits[0, -1].max().unsqueeze(0)

    try:
        # 5. [Î≥ÄÍ≤Ω] LayerIntegratedGradients -> IntegratedGradients
        # Î†àÏù¥Ïñ¥Í∞Ä ÏïÑÎãàÎùº 'ÏûÖÎ†• ÌÖêÏÑú(input_embeddings)' ÏûêÏ≤¥Î•º Î∂ÑÏÑù ÎåÄÏÉÅÏúºÎ°ú Ìï®
        ig = IntegratedGradients(forward_func)

        # 6. ÏÜçÏÑ±(Attribute) Í≥ÑÏÇ∞
        attributions, delta = ig.attribute(
            inputs=input_embeddings,
            # BaselineÏùÄ 0 Î≤°ÌÑ∞ (All-zero embeddings)
            baselines=torch.zeros_like(input_embeddings),
            n_steps=1,
            internal_batch_size=1, # Ìïú Î≤àÏóê 1Í∞ú Îã®Í≥ÑÏî©Îßå Í≥ÑÏÇ∞ÌïòÏó¨ Î©îÎ™®Î¶¨ Ï†àÏïΩ
            return_convergence_delta=True
        )
    except RuntimeError as e:
    #except (AcceleratorError, RuntimeError) as e:
    #except (cuda.Error, MemoryError, RuntimeError) as e:
      if "out of memory" in str(e):
        #del attributions, delta, inputs, input_ids, input_embeddings
        torch.cuda.empty_cache()
        print("Skipping sample due to OOM.")
        torch.cuda.empty_cache()
        return None, None, None, None
      else:
        raise e

    # 7. Ï§ëÏöîÎèÑ Ï†êÏàò Ìï©ÏÇ∞ (L2 Norm)
    scores = torch.norm(attributions, dim=-1).squeeze().tolist()
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])

    # 8. CDS Í≥ÑÏÇ∞ (Context ÏòÅÏó≠: 'Question:' Ï†ÑÍπåÏßÄ)
    try:
        # ÌÜ†ÌÅ∞ Ï§ë 'Question' Î¨∏ÏûêÏó¥ÏùÑ Ìè¨Ìï®ÌïòÎäî Ïù∏Îç±Ïä§ Ï∞æÍ∏∞
        sep_idx = next(i for i, t in enumerate(tokens) if "Question" in t)
        context_score = sum(scores[:sep_idx])
        total_score = sum(scores)
        cds = context_score / total_score if total_score > 0 else 0
    except StopIteration:
        cds = 0.5 # Íµ¨Î∂ÑÏûê Î™ª Ï∞æÏùå
        sep_idx = len(tokens) // 2

    del attributions, delta, inputs, input_ids, input_embeddings
    torch.cuda.empty_cache()
    #print(tokens, scores, cds, raw_attn)

    return tokens, scores, cds, raw_attn

! cp -r /root/.cache/huggingface/hub/datasets--ms_marco/. /content/drive/MyDrive/ColabNotebooks/hub/datasets--ms_marco/

! cp -r /root/.cache/huggingface/datasets/ms_marco/. /content/drive/MyDrive/ColabNotebooks/datasets/ms_marco/

! cp -r /root/.cache/huggingface/hub/datasets--squad/. /content/drive/MyDrive/ColabNotebooks/hub/datasets--squad/

! cp -r /root/.cache/huggingface/datasets/squad/. /content/drive/MyDrive/ColabNotebooks/datasets/squad/

# ==========================================
# 2. Î©îÏù∏ Ïã§Ìóò Î£®ÌîÑ (Îç∞Ïù¥ÌÑ∞ ÏàòÏßë) 3Ï¥à
# ==========================================

#dataset = load_dataset('hotpot_qa', 'distractor', split="validation", cache_dir="/content/drive/MyDrive/ColabNotebooks/datasets")
#dataset1 = load_dataset('squad', 'plain_text', split="validation", cache_dir="/content/drive/MyDrive/ColabNotebooks/datasets")
#dataset2 = load_dataset('ms_marco', 'v2.1', split="validation", cache_dir="/content/drive/MyDrive/ColabNotebooks/datasets")
#dataset2 = dataset2.rename_column("query", "question").rename_column("passages", "context")
dataset3 = load_dataset('hotpot_qa', 'distractor', split="validation", cache_dir="/content/drive/MyDrive/ColabNotebooks/hub")

#dataset1, dataset2,
dataset3

dataset3[0]['context']['sentences'][0]

dataset2[0]['context']['passage_text']

dataset1[0]['context']

results = []; NUM_SAMPLES = 200  # ÎÖºÎ¨∏Ïö©ÏúºÎ°úÎäî 100~200Í∞ú Í∂åÏû•(10Í∞ú/1Î∂Ñ)
best_viz_candidate = None
max_cds_diff = -1
dataset = dataset3.shuffle(seed=42).select(range(NUM_SAMPLES))
print(f"\nüöÄ Collecting Data from {NUM_SAMPLES} samples...")

for i in tqdm(range(NUM_SAMPLES)):
    print(i); torch.cuda.empty_cache()#; print(torch.cuda.memory_allocated(0))
    data = dataset[i]
    question = data['question']
    # [Ï°∞Í±¥ 1] Faithful: Ïò¨Î∞îÎ•∏ Î¨∏Îß•
    cds_faithful = " ".join(["".join(sent) for sent in data['context']['sentences']])
    #cds_faithful = " ".join(["".join(sent) for sent in data['context']['passage_text']])
    #cds_faithful= data['context']
    #print("cds_faithful:",cds_faithful)
    # [Ï°∞Í±¥ 2] Distracted: Î¨¥Í¥ÄÌïú Î¨∏Îß• (ÎûúÎç§ ÏÉòÌîåÎßÅ)
    rand_idx = random.randint(0, len(dataset)-1)
    while rand_idx == i: rand_idx = random.randint(0, len(dataset)-1)
    cds_distracted = " ".join(["".join(sent) for sent in dataset[rand_idx]['context']['sentences']])
    #cds_distracted = " ".join(["".join(sent) for sent in dataset[rand_idx]['context']['passage_text']])
    #cds_distracted = dataset[rand_idx]['context']
    #print("cds_distracted:",cds_distracted)

    # Î∂ÑÏÑù Ïã§Ìñâ
    tok_f, score_f, cds_f, attn_f = get_phi3_token_importance(cds_faithful, question, model, tokenizer)
    tok_h, score_h, cds_h, attn_h = get_phi3_token_importance(cds_distracted, question, model, tokenizer)
    if cds_f is None or cds_h is None: continue # ÏóêÎü¨/OOM Ïä§ÌÇµ

    # Í≤∞Í≥º Ï†ÄÏû•
    diff = cds_f - cds_h
    results.append({        "id": i,        "cds_faithful": cds_f,
                    "cds_distracted": cds_h,        "diff": diff    })
    # ÏãúÍ∞ÅÌôîÏö© 'ÏµúÍ≥†Ïùò ÏÉòÌîå' Ï†ÄÏû• (Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• ÌÅ∞ Í≤É)
    if diff > max_cds_diff:
      max_cds_diff = diff
      best_viz_candidate = {
          "tok_f": tok_f, "score_f": score_f, "attn_f": attn_f,
          "tok_h": tok_h, "score_h": score_h, "attn_h": attn_h,
          "q": question        }

# ==========================================
# 3. Ï†ïÎüâ Î∂ÑÏÑù: ÌÜµÍ≥Ñ Í≤ÄÏ†ï (T-test)
# ==========================================
df = pd.DataFrame(results)
t_stat, p_val = stats.ttest_rel(df['cds_faithful'], df['cds_hallucinated'])

print("\n" + "="*50)
print("üìä [Phase 1] Quantitative Results")
print("="*50)
print(f"Mean CDS (Faithful):     {df['cds_faithful'].mean():.4f} (std: {df['cds_faithful'].std():.4f})")
print(f"Mean CDS (Distracted): {df['cds_distracted'].mean():.4f} (std: {df['cds_distracted'].std():.4f})")
print(f"Gap (Faithful - Distracted): {df['diff'].mean():.4f}")
print("-" * 50)
print(f"Statistical Significance (Paired T-test):")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value:     {p_val:.4e}")
if p_val < 0.05:
    print("‚úÖ Result: Statistically Significant (p < 0.05)")
else:
    print("‚ùå Result: Not Significant")

# CSV Ï†ÄÏû•
df.to_csv("phase1_experiment_results.csv", index=False)

# ==========================================
# 4. Ï†ïÏÑ± Î∂ÑÏÑù ÏãúÍ∞ÅÌôî (Figure 1 & 2)
# ==========================================
if best_viz_candidate:
    sns.set_theme(style="white")
    plt.rcParams['font.family'] = 'serif'
    # --- Figure 1: Heatmap Comparison (Intensity) ---
    fig1, axes = plt.subplots(2, 1, figsize=(12, 5))

    def draw_heatmap(ax, tokens, scores, title):
      limit = 60
      clean = [t.replace(' ', '').replace('ƒ†', '') for t in tokens][:limit]
      vals = np.array(scores)[:limit].reshape(1, -1)
      vals = (vals - vals.min()) / (vals.max() - vals.min() + 1e-9) # Normalize

      sns.heatmap(vals, xticklabels=clean, yticklabels=False, cmap="Reds", ax=ax, cbar=False)
      ax.set_title(title, loc='left', fontsize=12, fontweight='bold')
      ax.set_xticklabels(clean, rotation=45, ha='right', fontsize=8)

    draw_heatmap(axes[0], best_viz_candidate['tok_f'], best_viz_candidate['score_f'], "(a) Faithful: High Attribution on Context")
    draw_heatmap(axes[1], best_viz_candidate['tok_h'], best_viz_candidate['score_h'], "(b) Hallucinated: Attribution Shifted to Question")

    plt.tight_layout()
    plt.savefig("fig1_heatmap.png", dpi=300)
    print("‚úÖ Figure 1 Saved.")

    # --- Figure 2: Attention Map Comparison (Structure) ---
    # Ïñ¥ÌÖêÏÖò ÎßµÏùÄ N x N Ïù¥ÎØÄÎ°ú ÏûòÎùºÏÑú(Crop) Î≥¥Ïó¨Ï£ºÎäî Í≤ÉÏù¥ Ï¢ãÏùå
    fig2, axes = plt.subplots(1, 2, figsize=(14, 6))

    def draw_attn_map(ax, tokens, attn_mat, title):
      limit = 40 # 40x40 ÌÜ†ÌÅ∞Îßå ÏãúÍ∞ÅÌôî (Í∞ÄÎèÖÏÑ± ÏúÑÌï®)
      clean = [t.replace(' ', '').replace('ƒ†', '') for t in tokens][:limit]
      mat = attn_mat[:limit, :limit]

      sns.heatmap(mat, xticklabels=clean, yticklabels=clean, cmap="Blues", ax=ax, cbar=False, square=True)
      ax.set_title(title, fontsize=12, fontweight='bold')
      ax.tick_params(axis='x', rotation=90, labelsize=7)
      ax.tick_params(axis='y', rotation=0, labelsize=7)

    draw_attn_map(axes[0], best_viz_candidate['tok_f'], best_viz_candidate['attn_f'], "(a) Faithful Attention Structure")
    draw_attn_map(axes[1], best_viz_candidate['tok_h'], best_viz_candidate['attn_h'], "(b) Hallucinated Attention Structure")

    plt.tight_layout()
    plt.savefig("fig2_attention_map.png", dpi=300)
    print("‚úÖ Figure 2 Saved.")

# ==========================================
# 4. Ï†ïÏÑ± Î∂ÑÏÑù: Figure 1 ÌûàÌä∏Îßµ ÏÉùÏÑ±
# ==========================================
if best_viz_candidate:
    print(f"\nüé® Generating Figure 1 from Sample ID (Best Gap)...")

    sns.set_theme(style="white")
    plt.rcParams['font.family'] = 'serif'
    fig, axes = plt.subplots(2, 1, figsize=(12, 5), sharex=False)
    def draw_row(ax, tokens, scores, title):
        # ÏãúÍ∞ÅÌôî Í∏∏Ïù¥ Ï†úÌïú
        limit = 70
        clean_toks = [t.replace(' ', '').replace('ƒ†', '') for t in tokens][:limit]
        norm_scores = np.array(scores)[:limit]
        # Ï†ïÍ∑úÌôî
        if norm_scores.max() > 0:
            norm_scores /= norm_scores.max()
        sns.heatmap(
            norm_scores.reshape(1, -1),
            xticklabels=clean_toks, yticklabels=False,
            cmap="Reds", ax=ax, cbar=True,
            cbar_kws={"orientation": "vertical", "shrink": 0.8}
        )
        ax.set_title(title, fontsize=12, fontweight='bold', loc='left')
        ax.set_xticklabels(clean_toks, rotation=45, ha='right', fontsize=8)
    draw_row(axes[0], best_viz_candidate['tok_f'], best_viz_candidate['score_f'],
             f"(a) Faithful Generation (Correct Context) - High Attention on Context")
    draw_row(axes[1], best_viz_candidate['tok_h'], best_viz_candidate['score_h'],
             f"(b) Hallucination Risk (Irrelevant Context) - Attention Shifted to Question")

    plt.tight_layout()
    plt.savefig("phase1_figure1_heatmap.png", bbox_inches='tight', dpi=300)
    print("‚úÖ Saved 'phase1_figure1_heatmap.png'")

print("\nüéâ Phase 1 Experiment Complete.")
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v67O-Un0eO8K"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ares2012/temp/blob/master/crawling.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMm0x1AcLPK"
      },
      "source": [
        "https://openincolab.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sxvfkzdRXln"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ssl\n",
        "\n",
        "context = ssl._create_unverified_context()\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6j4Dw7-9zGY"
      },
      "source": [
        "##### Google search\n",
        "\n",
        "https://jungwoon.github.io/python/2018/03/20/Data-Analysis-With-Python-3.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LCcoCazI_CX"
      },
      "outputs": [],
      "source": [
        "!pip install googlesearch-python --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "BBdkZvw-HX3P",
        "outputId": "7af90f81-9119-4928-c964-e18699f55aa0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(\"Google Search몇 초 안에 이동하지 않는 경우 여기를 클릭하세요.If you're having trouble accessing Google Search, pleaseclick here, or sendfeedback.\",)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://google.com/search?q=BHP'\n",
        "\n",
        "# Perform the request\n",
        "request = urllib.request.Request(url)\n",
        "\n",
        "# Set a normal User Agent header, otherwise Google will block the request.\n",
        "request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36')\n",
        "raw_response = urllib.request.urlopen(request, context=context).read()\n",
        "\n",
        "# Read the repsonse as a utf-8 string\n",
        "html = raw_response.decode(\"utf-8\")\n",
        "html\n",
        "\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "soup.text, #soup.prettify(), soup.p['class'], soup.b.prettify()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcFsRjOjI_CZ"
      },
      "outputs": [],
      "source": [
        "from googlesearch import search\n",
        "\n",
        "proxy = 'http://API:@proxy.host.com:8080/'\n",
        "\n",
        "j = search(\"proxy test\", num_results=100, lang=\"en\", proxy=proxy, ssl_verify=False)\n",
        "for i in j:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7WL7TsRI_Ca"
      },
      "outputs": [],
      "source": [
        "import urllib3\n",
        "\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "h3-IiwK--RKj",
        "outputId": "8a393ea7-6971-40e0-da69-e7e1e7ddd1f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([SearchResult(url=https://www.bhp.com/, title=BHP: Our products help build a better, clearer future, description= What we produce. Copper for renewable energy. Iron ore and metallurgical coal for steel for new infrastructure. And potash to support more sustainable farming. ),\n",
              "  SearchResult(url=https://twitter.com/bhp?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor, title=, description=),\n",
              "  SearchResult(url=https://finance.yahoo.com/quote/BHP/, title=BHP Group Limited (BHP) Stock Price, News, Quote & History, description= BHP Group Limited operates as a resources company in Australia, Europe, China, Japan, India, South Korea, the rest of Asia, North America, South America, ... ),\n",
              "  SearchResult(url=http://www.bhp.com/, title=, description= BHP Group Limited, also known as Broken Hill Proprietary Company and formerly as BHP Billiton is an Australian multinational mining and metals public company that was founded in August 1885 and is headquartered in Melbourne.   Wikipedia ),\n",
              "  SearchResult(url=/search?num=12, title=  BHP (Iron ore company)  , description= BHP (Iron ore company) ),\n",
              "  SearchResult(url=https://www.google.com/search?num=12, title=, description=),\n",
              "  SearchResult(url=https://en.wikipedia.org/wiki/BHP, title=BHP - Wikipedia, description= BHP Billiton is an Australian multinational mining and metals public company that was founded in August 1885 and is headquartered in Melbourne. ),\n",
              "  SearchResult(url=https://au.linkedin.com/company/bhp, title=BHP - LinkedIn, description= We are supplying the resources the world needs to help build a better, clearer future. Copper for renewable energy. Potash for sustainable farming. ),\n",
              "  SearchResult(url=https://www.youtube.com/BHP, title=BHP - YouTube, description= Our purpose is to bring people and resources together to build a better world. ...more ...more bhp.comand 4 more links. Subscribe. ),\n",
              "  SearchResult(url=https://www.asx.com.au/markets/company/BHP, title=BHP share price and company information for ASX:BHP, description= View today's BHP share price, options, bonds, hybrids and warrants. View announcements, advanced pricing charts, trading status, fundamentals, ... )],\n",
              " 'https://www.bhp.com/',\n",
              " 'BHP: Our products help build a better, clearer future',\n",
              " ' What we produce. Copper for renewable energy. Iron ore and metallurgical coal for steel for new infrastructure. And potash to support more sustainable farming. ')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from googlesearch import search\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "query = 'BHP'\n",
        "urls = []\n",
        "for url in search(query, num_results=10, advanced=True):\n",
        "  urls.append(url)\n",
        "\n",
        "urls, urls[0].url, urls[0].title, urls[0].description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emk1YLtVI_Ca",
        "outputId": "2faafb93-ce44-4276-9526-954c095edcf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "READ TIMED OUT - SearchResult(url=https://www.bhp.com/, title=BHP: Our products help build a better, clearer future, description= What we produce. Copper for renewable energy. Iron ore and metallurgical coal for steel for new infrastructure. And potash to support more sustainable farming. )\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>SearchResult(url=https://twitter.com/bhp?ref_s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>SearchResult(url=https://finance.yahoo.com/quo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Google</td>\n",
              "      <td>SearchResult(url=https://www.google.com/search...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BHP - Wikipedia</td>\n",
              "      <td>SearchResult(url=https://en.wikipedia.org/wiki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td></td>\n",
              "      <td>SearchResult(url=https://au.linkedin.com/compa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>BHP - YouTube</td>\n",
              "      <td>SearchResult(url=https://www.youtube.com/BHP, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td></td>\n",
              "      <td>SearchResult(url=https://www.asx.com.au/market...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             title                                                url\n",
              "0                   SearchResult(url=https://twitter.com/bhp?ref_s...\n",
              "1                   SearchResult(url=https://finance.yahoo.com/quo...\n",
              "2           Google  SearchResult(url=https://www.google.com/search...\n",
              "3  BHP - Wikipedia  SearchResult(url=https://en.wikipedia.org/wiki...\n",
              "4                   SearchResult(url=https://au.linkedin.com/compa...\n",
              "5    BHP - YouTube  SearchResult(url=https://www.youtube.com/BHP, ...\n",
              "6                   SearchResult(url=https://www.asx.com.au/market..."
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''불필요'''\n",
        "\n",
        "crawl_rst = dict()\n",
        "crawl_list = list()\n",
        "for url in urls[:]:\n",
        "  #print(url.url)\n",
        "  if (url.url).startswith('https://'):\n",
        "    try:\n",
        "      with requests.get(url.url, verify=False, timeout=3, stream=True) as url_result:\n",
        "        html = url_result.text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = soup.get_text()\n",
        "        #print(soup.title)\n",
        "        tt = ''\n",
        "        if soup.title is not None:\n",
        "          tt = soup.title.string\n",
        "          #print(soup.title.find_all(string=True))\n",
        "        #soup.b.prettify()\n",
        "        crawl_rst['title']=tt\n",
        "        crawl_rst['url']=url\n",
        "        crawl_list.append(crawl_rst.copy())\n",
        "        #print(crawl_list)\n",
        "    except requests.exceptions.ReadTimeout:\n",
        "      print (\"READ TIMED OUT -\", url)\n",
        "\n",
        "df=pd.DataFrame(crawl_list)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFjZbLBNGyiW"
      },
      "source": [
        "##### Google News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uaa_bQlzBZ3S",
        "outputId": "215de99f-3ba1-4ebb-af7f-34f58bcbb9d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install GoogleNews --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwDR0XhDBgLt",
        "outputId": "afb7af97-a28e-4523-859f-b604f3663594"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  DEPRECATION: tinysegmenter is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
            "  DEPRECATION: sgmllib3k is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
            "  DEPRECATION: jieba3k is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
            "  DEPRECATION: feedfinder2 is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install newspaper3k --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH0gDW1jB2Tl",
        "outputId": "01a69b37-0aa5-4c9b-ee79-0cacacf10b03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install lxml lxml_html_clean  --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "lwJl0gHlBXrs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               title  \\\n",
            "0               세계최대광산기업 BHP, 경쟁사 인수추진…세계구리 10% 생산가능   \n",
            "1                     Should I buy BHP shares today?   \n",
            "2          2025 MG Comet EV launched at Rs 4.99 lakh   \n",
            "3  BHP to Invest Up to AU$40 Million in Cobre’s K...   \n",
            "4  Australia: BHP earns right to take 75% stake i...   \n",
            "\n",
            "                       media   date  datetime  \\\n",
            "0                       연합뉴스  0개월 전       NaN   \n",
            "1  The Motley Fool Australia   2일 전       NaN   \n",
            "2                   Team-BHP  5시간 전       NaN   \n",
            "3     Investing News Network   1주 전       NaN   \n",
            "4                      ZAWYA   5일 전       NaN   \n",
            "\n",
            "                                                desc  \\\n",
            "0  '빅 오스트레일리아'라는 별명을 가진 BHP는 2001년 호주 BHP와 영국 빌리턴...   \n",
            "1  With copper prices rising and iron ore prices ...   \n",
            "2  The Comet EV now gets new features such as a r...   \n",
            "3  Among other deal terms, BHP will have the righ...   \n",
            "4  The deal follows Cobre's selection in BHP's Xp...   \n",
            "\n",
            "                                                link  \\\n",
            "0  https://www.yna.co.kr/view/AKR2024042515220010...   \n",
            "1  https://www.fool.com.au/2025/03/18/should-i-bu...   \n",
            "2  https://www.team-bhp.com/news/2025-mg-comet-ev...   \n",
            "3  https://investingnews.com/bhp-invest-cobre-cop...   \n",
            "4  https://www.zawya.com/en/projects/mining/austr...   \n",
            "\n",
            "                                                 img  \n",
            "0  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
            "1  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
            "2  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
            "3  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
            "4  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
            "                                               title                   media  \\\n",
            "5  ABB to deliver world’s largest production capa...   African Mining Market   \n",
            "6  BHP earns right to take 75% stake in Cobre's B...                 Reuters   \n",
            "7  BHP's Anglo miss sees playing second fiddle to...  www.mining-journal.com   \n",
            "8  Mothers demand justice as London case over Bra...            The Guardian   \n",
            "9  BHP to invest $25m in Cobre's Botswana copper ...           Mining Weekly   \n",
            "\n",
            "   date  datetime                                               desc  \\\n",
            "5  1주 전       NaN  ABB has been selected by global mining company...   \n",
            "6  1주 전       NaN  Mining conglomerate BHP has taken a right to a...   \n",
            "7  2일 전       NaN  BHP claims that last year's failed bid for Ang...   \n",
            "8  6일 전       NaN  Nineteen people died in the Mariana disaster, ...   \n",
            "9  1주 전       NaN  Mining major BHP has committed to investing up...   \n",
            "\n",
            "                                                link  \\\n",
            "5  https://africanminingmarket.com/abb-to-deliver...   \n",
            "6  https://www.reuters.com/markets/commodities/bh...   \n",
            "7  https://www.mining-journal.com/miner-s-right/o...   \n",
            "8  https://www.theguardian.com/business/2025/mar/...   \n",
            "9  https://www.miningweekly.com/article/bhp-to-in...   \n",
            "\n",
            "                                                 img  \n",
            "5  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
            "6  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
            "7  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
            "8  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
            "9  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nfor i in range(2,20):\\n    googlenews.getpage(i)\\n    result=googlenews.result()\\n    df=pd.DataFrame(result)\\n'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from GoogleNews import GoogleNews\n",
        "from newspaper import Article\n",
        "import pandas as pd\n",
        "\n",
        "search_term = \"BHP\"\n",
        "\n",
        "googlenews=GoogleNews(encode='utf-8', lang='kr', region='KR')\n",
        "\n",
        "# I suppose the date is in \"MM/dd/yyyy\" format...\n",
        "#googlenews=GoogleNews(start='05/01/2021',end='05/31/2021')\n",
        "googlenews.set_time_range('01/01/2025','03/20/2025')\n",
        "#googlenews=GoogleNews(period='7d')\n",
        "googlenews.search(search_term)\n",
        "\n",
        "#googlenews.set_topic('CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp1ZEdvU0JYQjBMVUpTR2dKQ1VpZ0FQAQ')\n",
        "#googlenews.set_section('CAQiS0NCQVNNZ29JTDIwdk1EWnVkR29TQlhCMExVSlNHZ0pDVWlJT0NBUWFDZ29JTDIwdk1ESjJlRFFxQ3dvSkVnZEdkWFJsWW05c0tBQSouCAAqKggKIiRDQkFTRlFvSUwyMHZNRFp1ZEdvU0JYQjBMVUpTR2dKQ1VpZ0FQAVAB')\n",
        "\n",
        "#googlenews.get_news()\n",
        "\n",
        "result=googlenews.result()\n",
        "df=pd.DataFrame(result)\n",
        "print(df.head())\n",
        "'''\n",
        "for i in range(2,20):\n",
        "    googlenews.getpage(i)\n",
        "    result=googlenews.result()\n",
        "    df=pd.DataFrame(result)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://boringariel.tistory.com/68\n",
        "https://scrapeops.io/python-web-scraping-playbook/newspaper3k/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "\n",
        "# selenium headless mode\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('headless')\n",
        "driver = webdriver.Chrome(options=options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n"
          ]
        },
        {
          "ename": "ReadTimeoutError",
          "evalue": "HTTPConnectionPool(host='localhost', port=54028): Read timed out. (read timeout=120)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mArticleException\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     18\u001b[39m article.download()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43marticle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m article.nlp()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\newspaper\\article.py:191\u001b[39m, in \u001b[36mArticle.parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mthrow_if_not_downloaded_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28mself\u001b[39m.doc = \u001b[38;5;28mself\u001b[39m.config.get_parser().fromstring(\u001b[38;5;28mself\u001b[39m.html)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\newspaper\\article.py:531\u001b[39m, in \u001b[36mArticle.throw_if_not_downloaded_verbose\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.download_state == ArticleDownloadState.FAILED_RESPONSE:\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[33m'\u001b[39m\u001b[33mArticle `download()` failed with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m on URL \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m %\n\u001b[32m    532\u001b[39m           (\u001b[38;5;28mself\u001b[39m.download_exception_msg, \u001b[38;5;28mself\u001b[39m.url))\n",
            "\u001b[31mArticleException\u001b[39m: Article `download()` failed with HTTPSConnectionPool(host='www.bhp.com', port=443): Max retries exceeded with url: /news/media-centre/releases/2025/02/bhp-chair-succession (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:992)'))) on URL https://www.bhp.com/news/media-centre/releases/2025/02/bhp-chair-succession",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\.pyenv\\pyenv-win\\versions\\3.11.1\\Lib\\http\\client.py:1374\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\.pyenv\\pyenv-win\\versions\\3.11.1\\Lib\\http\\client.py:318\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\.pyenv\\pyenv-win\\versions\\3.11.1\\Lib\\http\\client.py:279\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\.pyenv\\pyenv-win\\versions\\3.11.1\\Lib\\socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
            "\u001b[31mTimeoutError\u001b[39m: timed out",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m: \u001b[38;5;66;03m# ssl error\u001b[39;00m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ind)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     article.download(input_html=driver.page_source)\n\u001b[32m     31\u001b[39m     article.parse()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:454\u001b[39m, in \u001b[36mWebDriver.get\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    437\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[32m    438\u001b[39m \u001b[33;03m    tab.\u001b[39;00m\n\u001b[32m    439\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    452\u001b[39m \u001b[33;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:427\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m    425\u001b[39m         params[\u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.session_id\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcommand_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m    429\u001b[39m     \u001b[38;5;28mself\u001b[39m.error_handler.check_response(response)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:404\u001b[39m, in \u001b[36mRemoteConnection.execute\u001b[39m\u001b[34m(self, command, params)\u001b[39m\n\u001b[32m    402\u001b[39m trimmed = \u001b[38;5;28mself\u001b[39m._trim_large_entries(params)\n\u001b[32m    403\u001b[39m LOGGER.debug(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, command_info[\u001b[32m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:428\u001b[39m, in \u001b[36mRemoteConnection._request\u001b[39m\u001b[34m(self, method, url, body)\u001b[39m\n\u001b[32m    425\u001b[39m     body = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client_config.keep_alive:\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m     statuscode = response.status\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\urllib3\\_request_methods.py:143\u001b[39m, in \u001b[36mRequestMethods.request\u001b[39m\u001b[34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request_encode_url(\n\u001b[32m    136\u001b[39m         method,\n\u001b[32m    137\u001b[39m         url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m         **urlopen_kw,\n\u001b[32m    141\u001b[39m     )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43murlopen_kw\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\urllib3\\_request_methods.py:278\u001b[39m, in \u001b[36mRequestMethods.request_encode_body\u001b[39m\u001b[34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[39m\n\u001b[32m    274\u001b[39m     extra_kw[\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m].setdefault(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m, content_type)\n\u001b[32m    276\u001b[39m extra_kw.update(urlopen_kw)\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\urllib3\\poolmanager.py:443\u001b[39m, in \u001b[36mPoolManager.urlopen\u001b[39m\u001b[34m(self, method, url, redirect, **kw)\u001b[39m\n\u001b[32m    441\u001b[39m     response = conn.urlopen(method, url, **kw)\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m redirect_location = redirect \u001b[38;5;129;01mand\u001b[39;00m response.get_redirect_location()\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_e, (\u001b[38;5;167;01mOSError\u001b[39;00m, HTTPException)):\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n\u001b[32m    846\u001b[39m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\urllib3\\util\\retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_read_error(error):\n\u001b[32m    472\u001b[39m     \u001b[38;5;66;03m# Read retry?\u001b[39;00m\n\u001b[32m    473\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    476\u001b[39m         read -= \u001b[32m1\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\urllib3\\util\\util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value.__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     41\u001b[39m     value = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    534\u001b[39m     response = conn.getresponse()\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# Set properties that are used by the pooling layer.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\urllib3\\connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Is the error actually a timeout? Will raise a ReadTimeout or pass\"\"\"\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(err, \u001b[33m\"\u001b[39m\u001b[33merrno\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m err.errno \u001b[38;5;129;01min\u001b[39;00m _blocking_errnos:\n",
            "\u001b[31mReadTimeoutError\u001b[39m: HTTPConnectionPool(host='localhost', port=54028): Read timed out. (read timeout=120)"
          ]
        }
      ],
      "source": [
        "from newspaper import Config\n",
        "import nltk\n",
        "\n",
        "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
        "config = Config()\n",
        "config.browser_user_agent = user_agent\n",
        "config.request_timeout = 3\n",
        "config.http_success_only = False\n",
        "\n",
        "list=[]\n",
        "for ind in df.index[:]:\n",
        "    dict={}\n",
        "    news_url = df['link'][ind].split('&')[0]\n",
        "    try:\n",
        "        article = Article(news_url, config=config)\n",
        "        #response = requests.get(news_url, verify=False)\n",
        "        #article.download(input_html=response.text)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        article.nlp()\n",
        "        dict['Date']=df['date'][ind]\n",
        "        dict['Media']=df['media'][ind]\n",
        "        dict['Title']=article.title\n",
        "        dict['Article']=article.text\n",
        "        dict['Summary']=article.summary\n",
        "        list.append(dict)\n",
        "    except: # ssl error\n",
        "        print(ind)\n",
        "        driver.get(news_url)\n",
        "        article.download(input_html=driver.page_source)\n",
        "        article.parse()\n",
        "        #article.nlp()\n",
        "        dict['Date']=df['date'][ind]\n",
        "        dict['Media']=df['media'][ind]\n",
        "        dict['Title']=article.title\n",
        "        dict['Article']=article.text\n",
        "        dict['Summary']=article.summary\n",
        "        list.append(dict)\n",
        "        \n",
        "news_df=pd.DataFrame(list)\n",
        "#news_df.to_excel(\"articles.xlsx\")\n",
        "\n",
        "news_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "l2EkcD21I_Cc",
        "outputId": "85b493df-6a26-4fc9-dda3-359c5c11a78b"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "1",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:413\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._range.index(new_key)\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[31mValueError\u001b[39m: 1 is not in range",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#driver.quit()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mnews_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mArticle\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1124\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1236\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:415\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    413\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._range.index(new_key)\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
            "\u001b[31mKeyError\u001b[39m: 1"
          ]
        }
      ],
      "source": [
        "#driver.quit()\n",
        "\n",
        "news_df['Article'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "ename": "ArticleException",
          "evalue": "Article `download()` failed with HTTPSConnectionPool(host='fox13now.com', port=443): Max retries exceeded with url: /2013/12/30/new-year-new-laws-obamacare-pot-guns-and-drones/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:992)'))) on URL http://fox13now.com/2013/12/30/new-year-new-laws-obamacare-pot-guns-and-drones/",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mArticleException\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m article = Article(url)\n\u001b[32m      5\u001b[39m article.download()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43marticle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m article.authors, article.publish_date, article.text, article.top_image, article.movies\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\newspaper\\article.py:191\u001b[39m, in \u001b[36mArticle.parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mthrow_if_not_downloaded_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28mself\u001b[39m.doc = \u001b[38;5;28mself\u001b[39m.config.get_parser().fromstring(\u001b[38;5;28mself\u001b[39m.html)\n\u001b[32m    194\u001b[39m     \u001b[38;5;28mself\u001b[39m.clean_doc = copy.deepcopy(\u001b[38;5;28mself\u001b[39m.doc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\.py3111\\Lib\\site-packages\\newspaper\\article.py:531\u001b[39m, in \u001b[36mArticle.throw_if_not_downloaded_verbose\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[33m'\u001b[39m\u001b[33mYou must `download()` an article first!\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.download_state == ArticleDownloadState.FAILED_RESPONSE:\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ArticleException(\u001b[33m'\u001b[39m\u001b[33mArticle `download()` failed with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m on URL \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m %\n\u001b[32m    532\u001b[39m           (\u001b[38;5;28mself\u001b[39m.download_exception_msg, \u001b[38;5;28mself\u001b[39m.url))\n",
            "\u001b[31mArticleException\u001b[39m: Article `download()` failed with HTTPSConnectionPool(host='fox13now.com', port=443): Max retries exceeded with url: /2013/12/30/new-year-new-laws-obamacare-pot-guns-and-drones/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:992)'))) on URL http://fox13now.com/2013/12/30/new-year-new-laws-obamacare-pot-guns-and-drones/"
          ]
        }
      ],
      "source": [
        "from newspaper import Article\n",
        "\n",
        "url = 'http://fox13now.com/2013/12/30/new-year-new-laws-obamacare-pot-guns-and-drones/'\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "article.authors, article.publish_date, article.text, article.top_image, article.movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "article.nlp()\n",
        "\n",
        "article.keywords, article.summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import newspaper\n",
        "\n",
        "cnn_paper = newspaper.build('https://cnn.com')\n",
        "\n",
        "for article in cnn_paper.articles:\n",
        "     print(article.url)\n",
        "\n",
        "for category in cnn_paper.category_urls():\n",
        "     print(category)\n",
        "\n",
        "cnn_article = cnn_paper.articles[0]\n",
        "cnn_article.download()\n",
        "cnn_article.parse()\n",
        "cnn_article.nlp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from newspaper import fulltext\n",
        "\n",
        "html = requests.get(\"\").text\n",
        "text = fulltext(html)\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIHA01-oI_Cd",
        "outputId": "892f43b1-0041-4aef-8240-40674f4f9bbc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>media</th>\n",
              "      <th>date</th>\n",
              "      <th>datetime</th>\n",
              "      <th>desc</th>\n",
              "      <th>link</th>\n",
              "      <th>img</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025 MG Comet EV launched at Rs 4.99 lakh</td>\n",
              "      <td>Team-BHP</td>\n",
              "      <td>2 hours ago</td>\n",
              "      <td>2025-03-20 07:41:20.141401</td>\n",
              "      <td>The Comet EV now gets new features such as a r...</td>\n",
              "      <td>https://www.team-bhp.com/news/2025-mg-comet-ev...</td>\n",
              "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tobias Wagner and his electric trucking adventure</td>\n",
              "      <td>Team-BHP</td>\n",
              "      <td>3 hours ago</td>\n",
              "      <td>2025-03-20 06:41:20.143400</td>\n",
              "      <td>I have been following the youtube channel of T...</td>\n",
              "      <td>https://www.team-bhp.com/forum/electric-cars/2...</td>\n",
              "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ducati Scrambler Icon Dark launched at Rs 9.97...</td>\n",
              "      <td>Team-BHP</td>\n",
              "      <td>4 hours ago</td>\n",
              "      <td>2025-03-20 05:41:20.144400</td>\n",
              "      <td>Ducati has launched the Scrambler Icon Dark in...</td>\n",
              "      <td>https://www.team-bhp.com/news/ducati-scrambler...</td>\n",
              "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Maruti Suzuki Tour S based on new-gen Dzire la...</td>\n",
              "      <td>Team-BHP</td>\n",
              "      <td>4 hours ago</td>\n",
              "      <td>2025-03-20 05:41:20.145401</td>\n",
              "      <td>The Tour S is priced at Rs 6.79 lakh for the p...</td>\n",
              "      <td>https://www.team-bhp.com/news/maruti-suzuki-to...</td>\n",
              "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ultraviolette Tesseract surpasses 50,000 bookings</td>\n",
              "      <td>Team-BHP</td>\n",
              "      <td>7 hours ago</td>\n",
              "      <td>2025-03-20 02:41:20.146400</td>\n",
              "      <td>The Tesseract comes with three battery options...</td>\n",
              "      <td>https://www.team-bhp.com/news/ultraviolette-te...</td>\n",
              "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>TVS Apache RTX 300 ADV design patent filed in ...</td>\n",
              "      <td>Team-BHP</td>\n",
              "      <td>2 days ago</td>\n",
              "      <td>2025-03-18 09:44:52.985809</td>\n",
              "      <td>The patent image reveals a road-biased ADV tha...</td>\n",
              "      <td>https://www.team-bhp.com/news/tvs-apache-rtx-3...</td>\n",
              "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>8 mid-size bikes in India</td>\n",
              "      <td>India Today</td>\n",
              "      <td>2 days ago</td>\n",
              "      <td>2025-03-18 09:44:52.992763</td>\n",
              "      <td>The Royal Enfield Hunter 350 is powered by a 3...</td>\n",
              "      <td>https://www.indiatoday.in/visualstories/auto/8...</td>\n",
              "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>Limited edition Jeep Compass Sandstorm launche...</td>\n",
              "      <td>Team-BHP</td>\n",
              "      <td>2 days ago</td>\n",
              "      <td>2025-03-18 09:44:52.999803</td>\n",
              "      <td>Jeep has launched the Compass Sandstorm Editio...</td>\n",
              "      <td>https://www.team-bhp.com/news/limited-edition-...</td>\n",
              "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>Skoda Auto VW India achieves 5 lakh engine pro...</td>\n",
              "      <td>Team-BHP</td>\n",
              "      <td>2 days ago</td>\n",
              "      <td>2025-03-18 09:44:53.005811</td>\n",
              "      <td>Skoda Auto VW India has achieved a new product...</td>\n",
              "      <td>https://www.team-bhp.com/news/skoda-auto-vw-in...</td>\n",
              "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>Atria Investments Inc Has $376,000 Stake in BH...</td>\n",
              "      <td>Defense World</td>\n",
              "      <td>2 days ago</td>\n",
              "      <td>2025-03-18 09:44:53.012763</td>\n",
              "      <td>Read Atria Investments Inc Has $376000 Stake i...</td>\n",
              "      <td>https://www.defenseworld.net/2025/03/17/atria-...</td>\n",
              "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>180 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 title          media  \\\n",
              "0            2025 MG Comet EV launched at Rs 4.99 lakh       Team-BHP   \n",
              "1    Tobias Wagner and his electric trucking adventure       Team-BHP   \n",
              "2    Ducati Scrambler Icon Dark launched at Rs 9.97...       Team-BHP   \n",
              "3    Maruti Suzuki Tour S based on new-gen Dzire la...       Team-BHP   \n",
              "4    Ultraviolette Tesseract surpasses 50,000 bookings       Team-BHP   \n",
              "..                                                 ...            ...   \n",
              "175  TVS Apache RTX 300 ADV design patent filed in ...       Team-BHP   \n",
              "176                          8 mid-size bikes in India    India Today   \n",
              "177  Limited edition Jeep Compass Sandstorm launche...       Team-BHP   \n",
              "178  Skoda Auto VW India achieves 5 lakh engine pro...       Team-BHP   \n",
              "179  Atria Investments Inc Has $376,000 Stake in BH...  Defense World   \n",
              "\n",
              "            date                   datetime  \\\n",
              "0    2 hours ago 2025-03-20 07:41:20.141401   \n",
              "1    3 hours ago 2025-03-20 06:41:20.143400   \n",
              "2    4 hours ago 2025-03-20 05:41:20.144400   \n",
              "3    4 hours ago 2025-03-20 05:41:20.145401   \n",
              "4    7 hours ago 2025-03-20 02:41:20.146400   \n",
              "..           ...                        ...   \n",
              "175   2 days ago 2025-03-18 09:44:52.985809   \n",
              "176   2 days ago 2025-03-18 09:44:52.992763   \n",
              "177   2 days ago 2025-03-18 09:44:52.999803   \n",
              "178   2 days ago 2025-03-18 09:44:53.005811   \n",
              "179   2 days ago 2025-03-18 09:44:53.012763   \n",
              "\n",
              "                                                  desc  \\\n",
              "0    The Comet EV now gets new features such as a r...   \n",
              "1    I have been following the youtube channel of T...   \n",
              "2    Ducati has launched the Scrambler Icon Dark in...   \n",
              "3    The Tour S is priced at Rs 6.79 lakh for the p...   \n",
              "4    The Tesseract comes with three battery options...   \n",
              "..                                                 ...   \n",
              "175  The patent image reveals a road-biased ADV tha...   \n",
              "176  The Royal Enfield Hunter 350 is powered by a 3...   \n",
              "177  Jeep has launched the Compass Sandstorm Editio...   \n",
              "178  Skoda Auto VW India has achieved a new product...   \n",
              "179  Read Atria Investments Inc Has $376000 Stake i...   \n",
              "\n",
              "                                                  link  \\\n",
              "0    https://www.team-bhp.com/news/2025-mg-comet-ev...   \n",
              "1    https://www.team-bhp.com/forum/electric-cars/2...   \n",
              "2    https://www.team-bhp.com/news/ducati-scrambler...   \n",
              "3    https://www.team-bhp.com/news/maruti-suzuki-to...   \n",
              "4    https://www.team-bhp.com/news/ultraviolette-te...   \n",
              "..                                                 ...   \n",
              "175  https://www.team-bhp.com/news/tvs-apache-rtx-3...   \n",
              "176  https://www.indiatoday.in/visualstories/auto/8...   \n",
              "177  https://www.team-bhp.com/news/limited-edition-...   \n",
              "178  https://www.team-bhp.com/news/skoda-auto-vw-in...   \n",
              "179  https://www.defenseworld.net/2025/03/17/atria-...   \n",
              "\n",
              "                                                   img  \n",
              "0    data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
              "1    data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
              "2    data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
              "3    data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
              "4    data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
              "..                                                 ...  \n",
              "175  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
              "176  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
              "177  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
              "178  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
              "179  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
              "\n",
              "[180 rows x 7 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initial list of results - it will contain a list of dictionaries (dict).\n",
        "results = []\n",
        "\n",
        "# Contains the final results = news filtered by the criteria\n",
        "# (news that in their description contains the search term).\n",
        "final_results = []\n",
        "\n",
        "# Get first 4 pages with the results and append those results to the list - you can set any other range according to  your needs:\n",
        "for page in range(1,4):\n",
        "  googlenews.getpage(page) # Consider add an timer for avoid multiple calls and get \"HTTP Error 429: Too Many Requests\" error.\n",
        "  results.extend(googlenews.result())\n",
        "\n",
        "# Remove duplicates and include to the \"final_results\" list\n",
        "# only the news that includes in their description the search term:\n",
        "for item in results:\n",
        "  if (item not in final_results and (search_term in item[\"desc\"])):\n",
        "    final_results.append(item)\n",
        "\n",
        "# Build and show the final dataframe:\n",
        "df=pd.DataFrame(results)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJRNMZ4JI_Cd"
      },
      "source": [
        "##### melon with BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WOHbGe8I_Cd",
        "outputId": "d7142c7c-f662-4b4c-b697-f274bff4d8ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. G-DRAGON - TOO BAD (feat. Anderson .Paak)\n",
            "2. 조째즈 - 모르시나요(PROD.로코베리)\n",
            "3. 제니 (JENNIE) - like JENNIE\n",
            "4. G-DRAGON - HOME SWEET HOME (feat. 태양, 대성)\n",
            "5. IVE (아이브) - REBEL HEART\n",
            "6. WOODZ - Drowning\n",
            "7. 황가람 - 나는 반딧불\n",
            "8. aespa - Whiplash\n",
            "9. BOYNEXTDOOR - 오늘만 I LOVE YOU\n",
            "10. IVE (아이브) - ATTITUDE\n",
            "11. 로제 (ROSÉ) - APT.\n",
            "12. G-DRAGON - TAKE ME\n",
            "13. 로제 (ROSÉ) - toxic till the end\n",
            "14. DAY6 (데이식스) - HAPPY\n",
            "15. G-DRAGON - PO￦ER\n",
            "16. 로이킴 - 내게 사랑이 뭐냐고 물어본다면\n",
            "17. 황가람 - 미치게 그리워서\n",
            "18. PLAVE - Dash\n",
            "19. 이클립스 (ECLIPSE) - 소나기\n",
            "20. BABYMONSTER - DRIP\n",
            "21. KiiiKiii (키키) - I DO ME\n",
            "22. 이창섭 - 천상연\n",
            "23. DAY6 (데이식스) - Welcome to the Show\n",
            "24. aespa - Supernova\n",
            "25. DAY6 (데이식스) - 한 페이지가 될 수 있게\n",
            "26. Hearts2Hearts (하츠투하츠) - The Chase\n",
            "27. PLAVE - RIZZ\n",
            "28. 이무진 - 청춘만화\n",
            "29. 임영웅 - 사랑은 늘 도망가\n",
            "30. LE SSERAFIM (르세라핌) - HOT\n",
            "31. 순순희(지환) - 슬픈 초대장\n",
            "32. aespa - UP (KARINA Solo)\n",
            "33. PLAVE - Island\n",
            "34. AKMU (악뮤) - 어떻게 이별까지 사랑하겠어, 널 사랑하는 거지\n",
            "35. QWER - 내 이름 맑음\n",
            "36. PLAVE - Chroma Drift\n",
            "37. DAY6 (데이식스) - 예뻤어\n",
            "38. Lady Gaga - Die With A Smile\n",
            "39. PLAVE - 12:32 (A to T)\n",
            "40. 아이유 - Love wins all\n",
            "41. 임영웅 - 우리들의 블루스\n",
            "42. 재쓰비 (JAESSBEE) - 너와의 모든 지금\n",
            "43. QWER - 고민중독\n",
            "44. G-DRAGON - DRAMA\n",
            "45. NewJeans - How Sweet\n",
            "46. G-DRAGON - 무제(無題) (Untitled, 2014)\n",
            "47. 임영웅 - 온기\n",
            "48. 너드커넥션 (Nerd Connection) - 그대만 있다면 (여름날 우리 X 너드커넥션 (Nerd Connection))\n",
            "49. 성시경 - 너의 모든 순간\n",
            "50. 오반(OVAN) - Flower\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'}\n",
        "data = requests.get('https://www.melon.com/chart/', headers=headers, verify=False)\n",
        "\n",
        "soup = BeautifulSoup(data.text, 'html.parser')\n",
        "\n",
        "# print(soup)\n",
        "\n",
        "top_100 = soup.select('#lst50 > td > div') #tr로 이루어진 list\n",
        "\n",
        "for tr in top_100 :\n",
        "    rank = tr.select_one('span.rank')\n",
        "\n",
        "    if rank is not None:\n",
        "        print(rank.text, end=\". \")\n",
        "\n",
        "    title = tr.select_one('div > div.ellipsis.rank01 > span > a')\n",
        "\n",
        "    if title is not None:\n",
        "        artist = tr.select_one('div.ellipsis.rank02 > a').text\n",
        "        print(artist,'-', title.text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBZSGOWLJNAI"
      },
      "source": [
        "##### Selenium in colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "c9bEa36BJVVK",
        "outputId": "a763f981-9d28-459c-f1c0-7f26cb9a253e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.1\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'selenium'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpython --version\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Selenium을 확인하려면 다음과 같이 사용해야 합니다.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mselenium\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(selenium.__version__)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'selenium'"
          ]
        }
      ],
      "source": [
        "# 느낌표를 붙이면 Shell 명령어로 실행됩니다.\n",
        "!python --version\n",
        "\n",
        "# Selenium을 확인하려면 다음과 같이 사용해야 합니다.\n",
        "import selenium\n",
        "print(selenium.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y6qHymi6JeKC",
        "outputId": "654f2bfd-7d07-41c7-8b12-3d18200b0067"
      },
      "outputs": [],
      "source": [
        "# 코랩을 시작할 때 아래코드를 한 번 돌려줍니다.\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o7Yg8ozwJkxC"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.service import Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "service = Service(executable_path=r'/usr/bin/chromedriver')\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')        # Head-less 설정\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "#driver = webdriver.Chrome('chromedriver', options=options)\n",
        "driver = webdriver.Chrome(service=service, options=options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mlejHT1kKoHr"
      },
      "outputs": [],
      "source": [
        "!pip install chromedriver-autoinstaller --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6HbGxOfAKrNi"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "97GD56uwKlh6"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "import chromedriver_autoinstaller\n",
        "\n",
        "# setup chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless') # ensure GUI is off\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')  # set path to chromedriver as per your configuration\n",
        "chrome_options.add_argument('lang=ko_KR') # 한국어\n",
        "chrome_options.headless = True\n",
        "\n",
        "# set path to chromedriver as per your configuration\n",
        "chromedriver_autoinstaller.install()\n",
        "\n",
        "# set the target URL\n",
        "url = \"put-url-here-to-scrape\"\n",
        "\n",
        "# set up the webdriver\n",
        "driver = webdriver.Chrome(options=chrome_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3drMc1YUL7NR",
        "outputId": "dd28e89e-6cf9-40cb-d3ba-399ebba58adc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Google <selenium.webdriver.chrome.webdriver.WebDriver (session=\"4b073af0c26f23786afcb61fbd84f599\")>\n"
          ]
        }
      ],
      "source": [
        "url = \"https://www.google.com/\"  # set up the webdriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# driver 작동 테스트\n",
        "driver.get(url)\n",
        "driver.implicitly_wait(3) # element가 로드될 때까지 지정한 시간만큼 대기할 수 있도록 설정\n",
        "# driver.get_screenshot_as_file('google_screen.png')\n",
        "title = driver.title\n",
        "#driver.close()\n",
        "\n",
        "print(title, driver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxfL3sksUZYl",
        "outputId": "932b35ee-d019-4a7e-bc46-d5bbd6e5b2f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 클릭하기\n",
        "driver.find_element(By.XPATH,'//*[@id=\"APjFqb\"]').click()\n",
        "\n",
        "driver.save_screenshot('click.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrq43EDgSx-M",
        "outputId": "40b8f246-fe9b-43db-d0f6-e7c8e25b80a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 값 입력하기\n",
        "driver.find_element(By.XPATH,'//*[@id=\"APjFqb\"]').send_keys(\"tistory\")\n",
        "\n",
        "driver.save_screenshot('tistory.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOgImAo3Uly1",
        "outputId": "66d76e06-4ad6-4665-91eb-d5b993440fbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 키보드 입력하기\n",
        "driver.find_element(By.XPATH,'//*[@id=\"APjFqb\"]').send_keys(\"tistory\")\n",
        "#driver.find_element(By.XPATH,'//*[@id=\"APjFqb\"]').send_keys(Keys.ENTER)\n",
        "driver.find_element(By.XPATH,'//*[@id=\"APjFqb\"]').send_keys(Keys.RETURN)\n",
        "\n",
        "driver.save_screenshot('enter.png') #둘다 안먹힘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz_fdjBaTQhP",
        "outputId": "e7190717-44e6-487a-bba4-699a0e7c6a5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clk = driver.find_element(By.NAME, 'q')\n",
        "clk.send_keys(\"naver\")\n",
        "clk.submit()\n",
        "time.sleep(3)\n",
        "\n",
        "driver.save_screenshot('naver.png')\n",
        "\n",
        "#로봇이 아닙니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Gb8GhJcRMW-y"
      },
      "outputs": [],
      "source": [
        "# 자원회수 - 코드 실행 마지막에 quit driver\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyFBGpIbS6BM",
        "outputId": "1c671b1e-10ac-43fc-a364-84fd7fc6aa41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['484DFD2AB7D0E2718AE00F000F3D6B9B']\n",
            "484DFD2AB7D0E2718AE00F000F3D6B9B\n"
          ]
        }
      ],
      "source": [
        "print(driver.window_handles)\n",
        "print(driver.current_window_handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#service = Service(executable_path=r'.chromedriver.exe')\n",
        "url = \"https://www.google.com/\"  # set up the webdriver\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# driver 작동 테스트\n",
        "driver.get(url)\n",
        "\n",
        "clk = driver.find_element(By.NAME, 'q')\n",
        "clk.send_keys(\"naver\")\n",
        "clk.submit()\n",
        "\n",
        "#로봇이 아닙니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eyc5-ZB3SP4Z",
        "outputId": "7dfc05d5-89d5-4033-fbbe-2bf4a5e14d33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = driver.page_source\n",
        "soup = BeautifulSoup(html)\n",
        "\n",
        "v = soup.select('.yuRUbf')\n",
        "v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vOusizVSq77"
      },
      "outputs": [],
      "source": [
        "for i in v:\n",
        "    print(i.select_one('.LC20lb.DKV0Md').text)\n",
        "    print(i.a.attrs['href'])\n",
        "    print()\n",
        "\n",
        "print('done')\n",
        "\n",
        "#driver.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def selenium_scroll_option():\n",
        "  SCROLL_PAUSE_SEC = 3\n",
        "\n",
        "  # 스크롤 높이 가져옴\n",
        "  last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "  while True:\n",
        "    # 끝까지 스크롤 다운\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "    # 10초 대기\n",
        "    time.sleep(SCROLL_PAUSE_SEC)\n",
        "\n",
        "    # 스크롤 다운 후 스크롤 높이 다시 가져옴\n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "    if new_height == last_height:\n",
        "        break\n",
        "    last_height = new_height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "X6DykIPwMBy5",
        "outputId": "5aa69e7a-ed9d-426c-c0ca-75ba569876d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "검색할 키워드를 입력 : shark\n",
            "저장할 이미지 이름 : shark\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'WebDriver' object has no attribute 'find_elements_by_xpath'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-65fffe44104e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# 클래스를 찾고 해당 클래스의 src 리스트를 만들자\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mselenium_scroll_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 스크롤하여 이미지를 많이 확보\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_elements_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//*[@id=\"islmp\"]/div/div/div/div/div[3]/div[2]/input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 이미지 더보기 클릭\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mselenium_scroll_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_elements_by_xpath'"
          ]
        }
      ],
      "source": [
        "courbet = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "\n",
        "# 키워드 검색하기\n",
        "\n",
        "base_url = \"https://www.google.co.kr/imghp?hl=ko\" # 구글 이미지 검색\n",
        "keyword = input(\"검색할 키워드를 입력 : \")\n",
        "# print(type(a))\n",
        "image_name = input(\"저장할 이미지 이름 : \")\n",
        "\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "driver.get(base_url)\n",
        "\n",
        "# driver.find_element(\"xpath\",'//*[@id=\"APjFqb\"]') # 검색창 /html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/textarea\n",
        "# browser = driver.find_element(By.ID, \"APjFqb\")\n",
        "\n",
        "browser =  driver.find_element(By.NAME, \"q\")\n",
        "browser.clear()\n",
        "browser.send_keys(keyword)\n",
        "browser.send_keys(Keys.RETURN)\n",
        "\n",
        "\n",
        "# 클래스를 찾고 해당 클래스의 src 리스트를 만들자\n",
        "selenium_scroll_option() # 스크롤하여 이미지를 많이 확보\n",
        "#driver.find_elements_by_xpath('//*[@id=\"islmp\"]/div/div/div/div/div[3]/div[2]/input')[0].click() # 이미지 더보기 클릭\n",
        "#selenium_scroll_option()\n",
        "\n",
        "'''이미지 src요소를 리스트업해서 이미지 url 저장'''\n",
        "\n",
        "images = driver.find_elements(By.CSS_SELECTOR,\".rg_i.Q4LuWd\") #  클래스 네임에서 공백은 .을 찍어줌\n",
        "images_url = []\n",
        "for i in images:\n",
        "    if i.get_attribute('src')!= None :\n",
        "        images_url.append(i.get_attribute('src'))\n",
        "    else :\n",
        "        images_url.append(i.get_attribute('data-src'))\n",
        "driver.close()\n",
        "\n",
        "# 겹치는 이미지 url 제거\n",
        "print(len(pd.DataFrame(images_url)))\n",
        "print(\"전체 다운로드한 이미지 개수: {}\\n동일한 이미지를 제거한 이미지 개수: {}\".format(len(images_url), len(pd.DataFrame(images_url)[0].unique())))\n",
        "images_url=pd.DataFrame(images_url)[0].unique()\n",
        "\n",
        "#'''해당하는 파일에 이미지 다운로드'''\n",
        "#\n",
        "if image_name == 'shark' :\n",
        "   for t, url in enumerate(images_url, 0):\n",
        "      urlretrieve(url, shark + image_name + '_' + str(t) + '.jpg')\n",
        "   driver.close()\n",
        "#\n",
        "#elif image_name == 'whale' :\n",
        "#   for t, url in enumerate(images_url, 0):\n",
        "#      urlretrieve(url, whale + image_name + '_' + str(t) + '.jpg')\n",
        "#   driver.close()\n",
        "#\n",
        "#elif image_name == 'dolphin' :\n",
        "#   for t, url in enumerate(images_url, 0):\n",
        "#      urlretrieve(url, dolphin + image_name + '_' + str(t) + '.jpg')\n",
        "#   driver.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfVX5zaHGd2"
      },
      "source": [
        "##### GDELT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nuRJlCgJhIy",
        "outputId": "f5cfeb42-c1b0-4068-aaa7-18a0064f70a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4479,\n",
              " ['20250317.export.CSV.zip',\n",
              "  '20250316.export.CSV.zip',\n",
              "  '20250315.export.CSV.zip',\n",
              "  '20250314.export.CSV.zip',\n",
              "  '20250313.export.CSV.zip'])"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "import lxml.html as lh\n",
        "\n",
        "gdelt_base_url = 'http://data.gdeltproject.org/events/'\n",
        "\n",
        "# get the list of all the links on the gdelt file page\n",
        "page = requests.get(gdelt_base_url+'index.html')\n",
        "doc = lh.fromstring(page.content)\n",
        "link_list = doc.xpath(\"//*/ul/li/a/@href\")\n",
        "\n",
        "# separate out those links that begin with four digits\n",
        "file_list = [x for x in link_list if str.isdigit(x[0:4])]\n",
        "len(file_list), file_list[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "cCNImevPJlVi",
        "outputId": "5c5acd88-e6f6-4594-ba53-0ff50264ffeb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20250317.</span>export.CSV.zip\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;36m20250317.\u001b[0mexport.CSV.zip\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">extracting,\n",
              "</pre>\n"
            ],
            "text/plain": [
              "extracting,\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">parsing,\n",
              "</pre>\n"
            ],
            "text/plain": [
              "parsing,\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-137-5173b8b9490a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m# extract lines with our interest country code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mfips_country_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m51\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m37\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m44\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                     \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0moutfilecounter\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "import os.path\n",
        "import urllib\n",
        "import zipfile\n",
        "import glob\n",
        "import operator\n",
        "\n",
        "infilecounter = 0\n",
        "outfilecounter = 0\n",
        "\n",
        "local_path = './GDELT_Data/'\n",
        "#fips_country_code = 'UK'\n",
        "fips_country_code = 'AU'\n",
        "\n",
        "\n",
        "for compressed_file in file_list[infilecounter:]:\n",
        "    print(compressed_file)\n",
        "\n",
        "    # if we dont have the compressed file stored locally, go get it. Keep trying if necessary.\n",
        "    while not os.path.isfile(local_path+compressed_file):\n",
        "        print( 'downloading,',)\n",
        "        urllib.request.urlretrieve(url=gdelt_base_url+compressed_file,\n",
        "                           filename=local_path+compressed_file)\n",
        "\n",
        "    # extract the contents of the compressed file to a temporary directory\n",
        "    print( 'extracting,',)\n",
        "    z = zipfile.ZipFile(file=local_path+compressed_file, mode='r')\n",
        "    z.extractall(path=local_path+'tmp/')\n",
        "\n",
        "    # parse each of the csv files in the working directory,\n",
        "    print( 'parsing,',)\n",
        "    for infile_name in glob.glob(local_path+'tmp/*'):\n",
        "        outfile_name = local_path+'country/'+fips_country_code+'%04i.tsv'%outfilecounter\n",
        "\n",
        "        # open the infile and outfile\n",
        "        with open(infile_name, mode='r') as infile, open(outfile_name, mode='w') as outfile:\n",
        "            for line in infile:\n",
        "                # extract lines with our interest country code\n",
        "                if fips_country_code in operator.itemgetter(51, 37, 44)(line.split('\\t')):\n",
        "                    outfile.write(line)\n",
        "            outfilecounter +=1\n",
        "\n",
        "        # delete the temporary file\n",
        "        os.remove(infile_name)\n",
        "    infilecounter +=1\n",
        "    print( 'done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "GrjDwsm7KhW2",
        "outputId": "94e7f4c8-8198-458a-a58a-db743fa1b077"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'CSV.header.fieldids.xlsx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-71fd55693368>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get the GDELT field names from a helper file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m colnames = pd.read_excel('CSV.header.fieldids.xlsx', sheet_name='Sheet1', \n\u001b[0m\u001b[1;32m      6\u001b[0m                          index_col='Column ID', usecols=[0,1])['Field Name']\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CSV.header.fieldids.xlsx'"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Get the GDELT field names from a helper file\n",
        "colnames = pd.read_excel('CSV.header.fieldids.xlsx', sheet_name='Sheet1',\n",
        "                         index_col='Column ID', usecols=[0,1])['Field Name']\n",
        "\n",
        "# Build DataFrames from each of the intermediary files\n",
        "files = glob.glob(local_path+'country/'+fips_country_code+'*')\n",
        "DFlist = []\n",
        "for active_file in files:\n",
        "    print( active_file)\n",
        "    DFlist.append(pd.read_csv(active_file, sep='\\t', header=None, dtype=str,\n",
        "                              names=colnames, index_col=['GLOBALEVENTID']))\n",
        "\n",
        "# Merge the file-based dataframes and save a pickle\n",
        "DF = pd.concat(DFlist)\n",
        "DF.to_pickle(local_path+'backup'+fips_country_code+'.pickle')\n",
        "\n",
        "# once everythin is safely stored away, remove the temporary files\n",
        "for active_file in files:\n",
        "    os.remove(active_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
